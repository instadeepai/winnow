# -- Import
from winnow.calibration.calibration_features import (
    PrositFeatures,
    MassErrorFeature,
    RetentionTimeFeature,
    ChimericFeatures,
    BeamFeatures,
    DiffusionBeamFeatures,
)
from winnow.calibration.calibrator import ProbabilityCalibrator
from winnow.datasets.calibration_dataset import CalibrationDataset
from winnow.datasets.data_loaders import (
    InstaNovoDatasetLoader,
    MZTabDatasetLoader,
    PointNovoDatasetLoader,
    WinnowDatasetLoader,
)
from winnow.fdr.nonparametric import NonParametricFDRControl
from winnow.fdr.database_grounded import DatabaseGroundedFDRControl
from winnow.constants import RESIDUE_MASSES

from dataclasses import dataclass
from enum import Enum
import typer
from typing_extensions import Annotated
from typing import Union, Optional
import logging
from rich.logging import RichHandler
from pathlib import Path
import yaml
import pandas as pd


# --- Configuration ---
SEED = 42
MZ_TOLERANCE = 0.02
HIDDEN_DIM = 10
TRAIN_FRACTION = 0.1


class DataSource(Enum):
    """Source of a dataset to be used for calibration."""

    winnow = "winnow"
    instanovo = "instanovo"
    pointnovo = "pointnovo"
    mztab = "mztab"


@dataclass
class WinnowDatasetConfig:
    """Config for calibration datasets saved through `winnow`."""

    data_dir: Path


@dataclass
class InstaNovoDatasetConfig:
    """Config for calibration datasets generated by InstaNovo."""

    beam_predictions_path: Path
    spectrum_path: Path


@dataclass
class MZTabDatasetConfig:
    """Config for calibration datasets saved in MZTab format."""

    spectrum_path: Path
    predictions_path: Path


@dataclass
class PointNovoDatasetConfig:
    """Config for calibration datasets generated by PointNovo."""

    mgf_path: Path
    predictions_path: Path


class FDRMethod(Enum):
    """FDR estimation method."""

    database = "database-ground"
    winnow = "winnow"


# --- Logging Setup ---
logger = logging.getLogger(__name__)
logger.setLevel(logging.INFO)
# Prevent duplicate messages by disabling propagation and using only RichHandler
logger.propagate = False
if not logger.handlers:
    logger.addHandler(RichHandler())

app = typer.Typer(
    name="winnow",
    help="""
    Confidence calibration and FDR estimation for de novo peptide sequencing.
    """,
)


def load_dataset(
    data_source: DataSource, dataset_config_path: Path
) -> CalibrationDataset:
    """Load PSM dataset into a `CalibrationDataset` object.

    Args:
        data_source (DataSource): The source of the dataset
        dataset_config_path (Path): Path to a `.yaml` file containing arguments
                                    for the load method for the data source.

    Raises:
        TypeError: If `data_source` is not one of the supported data sources

    Returns:
        CalibrationDataset: A calibration dataset
    """
    logger.info(f"Loading dataset from {data_source}.")
    with open(dataset_config_path) as dataset_config_file:
        if data_source is DataSource.winnow:
            winnow_dataset_config = WinnowDatasetConfig(
                **yaml.safe_load(dataset_config_file)
            )
            dataset = WinnowDatasetLoader().load(
                data_path=Path(winnow_dataset_config.data_dir)
            )
        elif data_source is DataSource.instanovo:
            instanovo_dataset_config = InstaNovoDatasetConfig(
                **yaml.safe_load(dataset_config_file)
            )
            dataset = InstaNovoDatasetLoader().load(
                data_path=Path(instanovo_dataset_config.spectrum_path),
                predictions_path=Path(instanovo_dataset_config.beam_predictions_path),
            )
        elif data_source is DataSource.mztab:
            mztab_dataset_config = MZTabDatasetConfig(
                **yaml.safe_load(dataset_config_file)
            )
            dataset = MZTabDatasetLoader().load(
                data_path=Path(mztab_dataset_config.spectrum_path),
                predictions_path=Path(mztab_dataset_config.predictions_path),
            )
        elif data_source is DataSource.pointnovo:
            pointnovo_dataset_config = PointNovoDatasetConfig(
                **yaml.safe_load(dataset_config_file)
            )
            dataset = PointNovoDatasetLoader().load(
                data_path=Path(pointnovo_dataset_config.mgf_path),
                predictions_path=Path(pointnovo_dataset_config.predictions_path),
            )
        else:
            raise TypeError(
                f"Data source was {data_source}. Only 'instanovo', 'mztab' and 'pointnovo' are supported."
            )
    return dataset


def filter_dataset(dataset: CalibrationDataset) -> CalibrationDataset:
    """Filter out rows whose predictions are empty or contain unsupported PSMs.

    Args:
        dataset (CalibrationDataset): The dataset to be filtered

    Returns:
        CalibrationDataset: The filtered dataset
    """
    logger.info("Filtering dataset.")
    filtered_dataset = (
        dataset.filter_entries(
            # Filter out non-list predictions
            metadata_predicate=lambda row: not isinstance(row["prediction"], list),
        )
        # Filter out empty predictions
        .filter_entries(metadata_predicate=lambda row: not row["prediction"])
    )
    return filtered_dataset


def initialise_calibrator(
    learn_prosit_missing: bool = True,
    learn_chimeric_missing: bool = True,
    learn_retention_missing: bool = True,
) -> ProbabilityCalibrator:
    """Set up the probability calibrator with features.

    Args:
        learn_prosit_missing: Whether to learn from missing Prosit features. If False,
            errors will be raised when invalid spectra are encountered.
        learn_chimeric_missing: Whether to learn from missing chimeric features. If False,
            errors will be raised when invalid spectra are encountered.
        learn_retention_missing: Whether to learn from missing retention time features. If False,
            errors will be raised when invalid spectra are encountered.

    Returns:
        ProbabilityCalibrator: Configured calibrator with specified features.
    """
    calibrator = ProbabilityCalibrator(SEED)
    calibrator.add_feature(MassErrorFeature(residue_masses=RESIDUE_MASSES))
    calibrator.add_feature(
        PrositFeatures(
            mz_tolerance=MZ_TOLERANCE, learn_from_missing=learn_prosit_missing
        )
    )
    calibrator.add_feature(
        RetentionTimeFeature(
            hidden_dim=HIDDEN_DIM,
            train_fraction=TRAIN_FRACTION,
            learn_from_missing=learn_retention_missing,
        )
    )
    calibrator.add_feature(
        ChimericFeatures(
            mz_tolerance=MZ_TOLERANCE, learn_from_missing=learn_chimeric_missing
        )
    )
    calibrator.add_feature(BeamFeatures())
    calibrator.add_feature(DiffusionBeamFeatures())
    return calibrator


def apply_fdr_control(
    fdr_control: Union[NonParametricFDRControl, DatabaseGroundedFDRControl],
    dataset: CalibrationDataset,
    fdr_threshold: float,
    confidence_column: str,
) -> pd.DataFrame:
    """Apply FDR control to a dataset."""
    if isinstance(fdr_control, NonParametricFDRControl):
        fdr_control.fit(dataset=dataset.metadata[confidence_column])
        dataset.metadata = fdr_control.add_psm_pep(dataset.metadata, confidence_column)
    else:
        fdr_control.fit(
            dataset=dataset.metadata[confidence_column],
            residue_masses=RESIDUE_MASSES,
        )
    dataset.metadata = fdr_control.add_psm_fdr(dataset.metadata, confidence_column)
    dataset.metadata = fdr_control.add_psm_q_value(dataset.metadata, confidence_column)
    confidence_cutoff = fdr_control.get_confidence_cutoff(threshold=fdr_threshold)
    output_data = dataset.metadata
    output_data = output_data[output_data[confidence_column] >= confidence_cutoff]
    return output_data


def check_if_labelled(dataset: CalibrationDataset) -> None:
    """Check if the dataset contains a ground-truth column."""
    if "sequence" not in dataset.metadata.columns:
        raise ValueError(
            "Database-grounded FDR control can only be performed on annotated data."
        )


@app.command(name="train", help="Fit a calibration model.")
def train(
    data_source: Annotated[
        DataSource, typer.Option(help="The type of PSM dataset to be calibrated.")
    ],
    dataset_config_path: Annotated[
        Path,
        typer.Option(
            help="The path to the config with the specification of the calibration dataset."
        ),
    ],
    model_output_dir: Annotated[
        Path,
        typer.Option(
            help="The path to the directory where the fitted model checkpoint will be saved."
        ),
    ],
    dataset_output_path: Annotated[
        Path, typer.Option(help="The path to write the output to.")
    ],
    learn_prosit_missing: Annotated[
        bool,
        typer.Option(
            help="Whether to learn from missing Prosit features. If False, training will fail if any spectra have invalid Prosit predictions."
        ),
    ] = True,
    learn_chimeric_missing: Annotated[
        bool,
        typer.Option(
            help="Whether to learn from missing chimeric features. If False, training will fail if any spectra have invalid predictions for chimeric feature computation."
        ),
    ] = True,
    learn_retention_missing: Annotated[
        bool,
        typer.Option(
            help="Whether to learn from missing retention time features. If False, training will fail if any spectra have invalid retention time predictions."
        ),
    ] = True,
):
    """Fit the calibration model.

    Args:
        data_source (Annotated[ DataSource, typer.Option, optional): The type of PSM dataset to be calibrated.
        dataset_config_path (Annotated[ Path, typer.Option, optional): The path to the config with the specification of the calibration dataset.
        model_output_dir (Annotated[Path, typer.Option, optional]): The path to the directory where the fitted model checkpoint will be saved.
        dataset_output_path (Annotated[Path, typer.Option, optional): The path to write the output to.
    """
    # -- Load dataset
    logger.info("Loading datasets.")
    annotated_dataset = load_dataset(
        data_source=data_source,
        dataset_config_path=dataset_config_path,
    )

    annotated_dataset = filter_dataset(annotated_dataset)

    # Train
    logger.info("Training calibrator.")
    calibrator = initialise_calibrator(
        learn_prosit_missing=learn_prosit_missing,
        learn_chimeric_missing=learn_chimeric_missing,
        learn_retention_missing=learn_retention_missing,
    )
    calibrator.fit(annotated_dataset)

    # -- Write model checkpoints
    logger.info(f"Saving model to {model_output_dir}")
    ProbabilityCalibrator.save(calibrator, model_output_dir)

    # -- Write output
    logger.info("Writing output.")
    annotated_dataset.to_csv(dataset_output_path)
    logger.info(f"Training dataset results saved: {dataset_output_path}")


@app.command(
    name="predict",
    help="Calibrate scores and optionally filter results to a target FDR.",
)
def predict(
    data_source: Annotated[
        DataSource, typer.Option(help="The type of PSM dataset to be calibrated.")
    ],
    dataset_config_path: Annotated[
        Path,
        typer.Option(
            help="The path to the config with the specification of the calibration dataset."
        ),
    ],
    method: Annotated[
        FDRMethod, typer.Option(help="Method to use for FDR estimation.")
    ],
    fdr_threshold: Annotated[
        float,
        typer.Option(
            help="The target FDR threshold (e.g. 0.01 for 1%, 0.05 for 5% etc.)"
        ),
    ],
    confidence_column: Annotated[
        str, typer.Option(help="Name of the column with confidence scores.")
    ],
    output_folder: Annotated[
        Path, typer.Option(help="The folder path to write the outputs to.")
    ],
    huggingface_model_name: Annotated[
        str,
        typer.Option(
            help="HuggingFace model identifier. If neither this nor `--local-model-folder` are provided, loads default model from HuggingFace.",
        ),
    ] = "InstaDeepAI/winnow-general-model",
    local_model_folder: Annotated[
        Optional[Path],
        typer.Option(
            help="Path to local calibrator directory. If neither this nor `--huggingface-model-name` are provided, loads default pretrained model from HuggingFace.",
        ),
    ] = None,
):
    """Calibrate model scores, estimate FDR and filter for a threshold.

    Args:
        data_source (Annotated[ DataSource, typer.Option, optional): The type of PSM dataset to be calibrated.
        dataset_config_path (Annotated[ Path, typer.Option, optional): The path to the config with the specification of the dataset.
        method (Annotated[ FDRMethod, typer.Option, optional): Method to use for FDR estimation.
        fdr_threshold (Annotated[ float, typer.Option, optional): The target FDR threshold (e.g. 0.01 for 1%, 0.05 for 5% etc.).
        confidence_column (Annotated[ str, typer.Option, optional): Name of the column with confidence scores.
        output_folder (Annotated[ Path, typer.Option, optional): The folder path to write the outputs to: `metadata.csv` and `preds_and_fdr_metrics.csv`.
        huggingface_model_name (Annotated[str, typer.Option, optional): HuggingFace model identifier.
        local_model_folder (Annotated[Path, typer.Option, optional): Path to local calibrator directory (e.g., Path("./my-model-directory")).

    Note that either `local_model_folder` or `huggingface-model-name` may be overwritten, but not both.
    If neither `local_model_folder` nor `huggingface-model-name` are provided, the general model from HuggingFace will be loaded by default (i.e., `InstaDeepAI/winnow-general-model`).
    """
    # -- Load dataset
    logger.info("Loading datasets.")
    dataset = load_dataset(
        data_source=data_source,
        dataset_config_path=dataset_config_path,
    )

    dataset = filter_dataset(dataset)

    # Predict
    # If local_model_folder is an empty string, load the HuggingFace model
    if local_model_folder is None:
        logger.info(f"Loading HuggingFace model: {huggingface_model_name}")
        calibrator = ProbabilityCalibrator.load(huggingface_model_name)
    # Otherwise, load the model from the local folder path
    else:
        logger.info(f"Loading local model from: {local_model_folder}")
        calibrator = ProbabilityCalibrator.load(local_model_folder)

    logger.info("Calibrating scores.")
    calibrator.predict(dataset)

    if method is FDRMethod.winnow:
        logger.info("Applying FDR control.")
        dataset_metadata = apply_fdr_control(
            NonParametricFDRControl(), dataset, fdr_threshold, confidence_column
        )
    elif method is FDRMethod.database:
        logger.info("Applying FDR control.")
        check_if_labelled(dataset)
        dataset_metadata = apply_fdr_control(
            DatabaseGroundedFDRControl(confidence_feature=confidence_column),
            dataset,
            fdr_threshold,
            confidence_column,
        )

    # -- Write output
    logger.info("Writing output.")
    # Separate out metadata from prediction and FDR metrics
    preds_and_fdr_metrics_cols = [
        confidence_column,
        "prediction",
        "psm_fdr",
        "psm_q_value",
    ]
    if "sequence" in dataset_metadata.columns:
        preds_and_fdr_metrics_cols.append("sequence")
    if method is FDRMethod.winnow:
        preds_and_fdr_metrics_cols.append("psm_pep")
    dataset_preds_and_fdr_metrics = dataset_metadata[
        preds_and_fdr_metrics_cols + ["spectrum_id"]
    ]
    dataset_metadata = dataset_metadata.drop(columns=preds_and_fdr_metrics_cols)
    # Write outputs
    output_folder.mkdir(parents=True)
    dataset_metadata.to_csv(output_folder / "metadata.csv")
    dataset_preds_and_fdr_metrics.to_csv(output_folder / "preds_and_fdr_metrics.csv")
    logger.info(f"Outputs saved: {output_folder}")


if __name__ == "__main__":
    app()
