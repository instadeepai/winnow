{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Winnow: *de novo* sequencing confidence calibration and FDR control tutorial\n",
    "\n",
    "<a target=\"_blank\" href=\"https://colab.research.google.com/github/instadeepai/winnow/blob/main/notebooks/getting_started_with_instanovo.ipynb\">\n",
    "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
    "</a>\n",
    "\n",
    "This notebook demonstrates how to use the `winnow` package for confidence calibration and false discovery rate (FDR) control in *de novo* sequencing (DNS) workflows.\n",
    "\n",
    "In the rest of the notebook, you'll:\n",
    "- Set up the environment and download example datasets.\n",
    "- Load and filter spectra and beam predictions; inspect key metadata.\n",
    "- Configure Winnow features (Mass Error, Prosit spectral/iRT, Chimeric, Beam) and train a calibrator on the HeLa Single Shot dataset.\n",
    "- Evaluate calibration with precisionâ€“recall curves, score transformation, confidence distributions, and calibration curves.\n",
    "- Control FDR via two approaches: database-grounded and label-free non-parametric; add per-PSM FDR/PEP/q-values and plot PSM FDR vs. confidence.\n",
    "- Apply the trained calibrator and label-free FDR to raw, unlabelled HeLa Single Shot data and filter results by a 5% FDR cutoff.\n",
    "- Extend Winnow: implement a custom feature, save/reload a calibrator, and run the workflow via the CLI.\n",
    "- Use a minimal (Prosit-free) feature set when Prosit isn't available.\n",
    "- Load a pretrained general calibrator and reproduce results on another dataset (Immunopeptidomics-2).\n",
    "- Review tips, troubleshooting notes, and additional resources."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../assets/winnow-pipeline.png\" alt=\"winnow pipeline\" width=\"600\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "`winnow` implements a calibrateâ€‘estimate framework for FDR control in *de novo* peptide sequencing. The flow:\n",
    "\n",
    "1. **Confidence calibration**: Learn a mapping from raw model confidence and auxiliary features to an improved, betterâ€‘calibrated probability of correctness.\n",
    "2. **FDR estimation**: Estimate/control FDR either with labels (databaseâ€‘grounded) or without labels via a non-parametric procedure.\n",
    "\n",
    "## Links\n",
    "\n",
    "- **Paper**: TODO\n",
    "- **Code**: [GitHub](https://github.com/instadeepai/winnow)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and imports\n",
    "\n",
    "First, let's import all the necessary packages and set up logging."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from winnow.calibration.calibration_features import (\n",
    "    CalibrationFeatures,\n",
    "    PrositFeatures,\n",
    "    MassErrorFeature,\n",
    "    RetentionTimeFeature,\n",
    "    ChimericFeatures,\n",
    "    BeamFeatures,\n",
    ")\n",
    "from winnow.calibration.calibrator import ProbabilityCalibrator\n",
    "from winnow.datasets.calibration_dataset import RESIDUE_MASSES, CalibrationDataset\n",
    "from winnow.fdr.database_grounded import DatabaseGroundedFDRControl\n",
    "from winnow.fdr.nonparametric import NonParametricFDRControl\n",
    "\n",
    "import logging\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "from huggingface_hub import snapshot_download, list_repo_files\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Suppress UserWarnings from winnow about FDR control extrapolation\n",
    "warnings.filterwarnings(\"ignore\", module=\"winnow\")\n",
    "\n",
    "sns.set_theme(style=\"whitegrid\", context=\"notebook\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- Set up logging\n",
    "logger = logging.getLogger(__name__)\n",
    "logger.setLevel(logging.INFO)\n",
    "logger.addHandler(logging.StreamHandler())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data download\n",
    "\n",
    "Let's download a HeLa single cell dataset and its corresponding InstaNovo predictions from [Hugging Face](https://huggingface.co/datasets/JemmaDaniel/winnow-ms-datasets)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "repo_id = \"InstaDeepAI/winnow-ms-datasets\"\n",
    "save_dir = \"winnow-ms-datasets\"\n",
    "\n",
    "files = list_repo_files(repo_id=repo_id, repo_type=\"dataset\")\n",
    "print([f for f in files])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- Download the helaqc dataset\n",
    "snapshot_download(\n",
    "    repo_id=repo_id,\n",
    "    repo_type=\"dataset\",\n",
    "    allow_patterns=[\"helaqc*.parquet\", \"helaqc*.csv\"],\n",
    "    local_dir=save_dir,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data loading and preparation\n",
    "\n",
    "In a typical workflow, you'll have:\n",
    "- **Spectrum data**: MS/MS spectra with metadata (charge, m/z, retention time)\n",
    "- **Beam predictions**: Peptide sequence predictions from your *de novo* model\n",
    "- **Annotations (Optional)**: Ground truth labels (for training/evaluation)\n",
    "\n",
    "If you do not have beam predictions (i.e., traditional search enginers or DNS models without beam search), you can submit a predictions list with just a single prediction per spectrum and its associated log-probabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- Load data\n",
    "logger.info(\"Loading dataset.\")\n",
    "dataset = CalibrationDataset.from_predictions_csv(\n",
    "    spectrum_path=\"winnow-ms-datasets/helaqc_labelled.parquet\",\n",
    "    beam_predictions_path=\"winnow-ms-datasets/helaqc_labelled_beams.csv\",\n",
    ")\n",
    "\n",
    "logger.info(\"Filtering dataset.\")\n",
    "filtered_dataset = (\n",
    "    dataset.filter_entries(\n",
    "        metadata_predicate=lambda row: not isinstance(row[\"prediction\"], list),\n",
    "    )\n",
    "    .filter_entries(metadata_predicate=lambda row: not row[\"prediction\"])\n",
    "    .filter_entries(\n",
    "        metadata_predicate=lambda row: row[\"precursor_charge\"] > 6\n",
    "    )  # Prosit-specific filtering, see https://github.com/Nesvilab/FragPipe/issues/1775\n",
    "    .filter_entries(\n",
    "        metadata_predicate=lambda row: len(row[\"prediction\"]) > 30\n",
    "    )  # Prosit-specific filtering\n",
    "    .filter_entries(\n",
    "        predictions_predicate=lambda row: len(row[1].sequence) > 30\n",
    "    )  # Prosit-specific filtering\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- Split data into train and test sets\n",
    "train_dataset = filtered_dataset.filter_entries(\n",
    "    metadata_predicate=lambda row: row[\"split\"] == \"test\"\n",
    ")\n",
    "test_dataset = filtered_dataset.filter_entries(\n",
    "    metadata_predicate=lambda row: row[\"split\"] == \"train\"\n",
    ")\n",
    "\n",
    "print(\"Number of spectra in train set:\", len(train_dataset))\n",
    "print(\"Number of spectra in test set:\", len(test_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset.metadata.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metadata columns\n",
    "\n",
    "Each row corresponds to an MS2 spectrum with metadata and arrays:\n",
    "- `spectrum_id` (string): unique spectrum identifier, constructed by concatenating experiment name with scan number\n",
    "- `sequence_untokenised` (string): ground truth peptide sequence from database search, for labelled data only, as a string\n",
    "- `prediction_untokenised` (string): model-predicted peptide sequence as a string\n",
    "- `prediction` (list(string)): model-predicted peptide sequence as a list of amino acids \n",
    "- `confidence` (float): model confidence for the top prediction, ranges between 0 and 1\n",
    "- `token_log_probs` (list(float)): amino acid-level log-probabilities\n",
    "- `sequence` (string): ground truth peptide sequence from database search, for labelled data only\n",
    "- `precursor_charge` (int): charge of the precursor (from MS1)\n",
    "- `precursor_mass` (float): mass of the precursor ion (from MS1)\n",
    "- `retention_time` (float): retention time (seconds)\n",
    "- `mz_array` (list[float]): mass-to-charge values of the MS2 spectrum\n",
    "- `intensity_array` (list[float]): intensity values of the MS2 spectrum\n",
    "- `split` (string): train, test or validation split identity for the MS2 spectrum (not required by Winnow)\n",
    "- `valid_peptide` (boolean): whether the entry in `sequence` is of type list\n",
    "- `valid_prediction` (boolean): whether the entry in `prediction` is of type list\n",
    "- `num_matches` (integer): number of token-level matches between the sequence and prediction, if labelled\n",
    "- `correct` (boolean): whether the predicted peptide sequence is correct (i.e., do all tokens match)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the first 3 beam results for the first spectrum\n",
    "train_dataset.predictions[0][0:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Feature engineering deep dive\n",
    "\n",
    "Below we describe the builtâ€‘in features that Winnow can use as complementary signals during the calibration step. Which features are informative depends on the dataset, instrument, acquisition settings and the upstream DNS model. Use the descriptions below to decide what to enable, and validate with ablations on your data.\n",
    "\n",
    "### Mass error\n",
    "What it captures: Consistency between the observed precursor mass and the theoretical mass of the predicted peptide (accounting for H2O and a proton).\n",
    "- Inputs used: `prediction`, `precursor_mass`, plus `RESIDUE_MASSES` for theoretical mass.\n",
    "- Metadata columns created: `Mass Error`.\n",
    "\n",
    "### Prosit features\n",
    "What it captures: How well Prositâ€‘predicted fragment ions explain the experimental spectrum.\n",
    "- Implemented as: fraction of matched ions and average matched intensity within an m/z tolerance.\n",
    "- Inputs used: `prediction`, `mz_array`, `intensity_array`, `precursor_charge`.\n",
    "- Metadata columns created: `prosit_mz`, `prosit_intensity`, `ion_matches`, `ion_match_intensity`.\n",
    "\n",
    "### Prosit iRT features\n",
    "What it captures: Agreement between Prosit iRT (sequenceâ€‘based) and observed retention time, via a small RT calibration model trained on highâ€‘confidence PSMs.\n",
    "- Inputs used: `prediction`, `retention_time`.\n",
    "- Metadata columns created: `iRT`, `predicted iRT`, `iRT error`.\n",
    "\n",
    "### Chimeric features\n",
    "What it captures: Whether runnerâ€‘up sequences also explain the spectrum (a proxy for chimericity).\n",
    "- Implemented as: ion match rate/intensity for the secondâ€‘best sequence.\n",
    "- Inputs used: `prediction` beams, `mz_array`, `intensity_array`, `precursor_charge`.\n",
    "- Metadata columns created: `runner_up_prosit_mz`, `runner_up_prosit_intensity`, `chimeric_ion_matches`, `chimeric_match_intensity`.\n",
    "\n",
    "### Beam features\n",
    "What it captures: Properties of the decoding distribution (margin to runnerâ€‘ups, median margin, entropy, and a zâ€‘score of the top beam relative to its peers).\n",
    "- Inputs used: beam logâ€‘probabilities from the upstream model.\n",
    "- Metadata columns created: `margin`, `median_margin`, `entropy`, `z-score`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Supported modifications and sequence notation\n",
    "\n",
    "Winnow expects tokenised peptide sequences where modifications are encoded inline using UNIMOD bracket notation. Examples:\n",
    "- Tokenised format: `A, C[UNIMOD:4], M[UNIMOD:35], Y[UNIMOD:21]`\n",
    "- Untokenised example (for display): `AC[UNIMOD:4]M[UNIMOD:35]Y[UNIMOD:21]`\n",
    "\n",
    "Supported modification tokens (parsed via `RESIDUE_MASSES`):\n",
    "- `C[UNIMOD:4]`: Carbamidomethylation (fixed, C)\n",
    "- `M[UNIMOD:35]`: Oxidation (M)\n",
    "- `N[UNIMOD:7]`, `Q[UNIMOD:7]`: Deamidation (N/Q)\n",
    "- `S[UNIMOD:21]`, `T[UNIMOD:21]`, `Y[UNIMOD:21]`: Phosphorylation (S/T/Y)\n",
    "- `[UNIMOD:1]`: Acetylation (e.g., N-term)\n",
    "- `[UNIMOD:5]`: Carbamylation\n",
    "- `[UNIMOD:385]`: Ammonia loss\n",
    "\n",
    "Accepted legacy/alternate notations are autoâ€‘remapped to UNIMOD tokens during loading (via `RESIDUE_REMAPPING`):\n",
    "- `M(ox)`, `M(+15.99)` â†’ `M[UNIMOD:35]`\n",
    "- `S(p)`, `T(p)`, `Y(p)`, `S(+79.97)`, `T(+79.97)`, `Y(+79.97)` â†’ `*[UNIMOD:21]`\n",
    "- `Q(+0.98)`, `N(+0.98)`, `Q(+.98)`, `N(+.98)` â†’ `*[UNIMOD:7]`\n",
    "- `C(+57.02)` â†’ `C[UNIMOD:4]`\n",
    "- `(+42.01)` â†’ `[UNIMOD:1]`; `(+43.01)` â†’ `[UNIMOD:5]`; `(-17.03)` â†’ `[UNIMOD:385]`\n",
    "\n",
    "Notation and preprocessing details:\n",
    "- Sequences are tokenised as commaâ€‘separated tokens in the dataset (e.g., `preds_tokenised`), and Winnow normalises `L` to `I` on load.\n",
    "- For Prositâ€‘based features (spectral and iRT): only peptides with supported modifications are queried. Rows with unsupported modifications are filtered. Carbamidomethylation on C (`C[UNIMOD:4]`) is treated as fixed; when querying Prosit, this is internally mapped to `C`.\n",
    "- If many rows are filtered during Prosit feature computation, review your modification tokens and acquisition settings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Calibration workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- Set up calibrator\n",
    "logger.info(\"Initialising calibrator.\")\n",
    "SEED = 42\n",
    "calibrator = ProbabilityCalibrator(SEED)\n",
    "\n",
    "logger.info(\"Adding features to calibrator.\")\n",
    "MZ_TOLERANCE = 0.02\n",
    "HIDDEN_DIM = 10\n",
    "TRAIN_FRACTION = 0.1\n",
    "calibrator.add_feature(MassErrorFeature(residue_masses=RESIDUE_MASSES))\n",
    "calibrator.add_feature(PrositFeatures(mz_tolerance=MZ_TOLERANCE))\n",
    "calibrator.add_feature(\n",
    "    RetentionTimeFeature(hidden_dim=HIDDEN_DIM, train_fraction=TRAIN_FRACTION)\n",
    ")\n",
    "calibrator.add_feature(ChimericFeatures(mz_tolerance=MZ_TOLERANCE))\n",
    "calibrator.add_feature(BeamFeatures())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- Calibrate\n",
    "logger.info(\"Calibrating scores.\")\n",
    "calibrator.fit(train_dataset)\n",
    "calibrator.predict(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset.metadata.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Added feature columns\n",
    "\n",
    "- `Mass Error` (float): difference between observed precursor mass and theoretical peptide mass, accounting for the mass of water and a proton.\n",
    "- `prosit_mz` (list(float)): Prosit-predicted m/z values\n",
    "- `prosit_intensity` (list(float)): Prosit-predicted intensity values\n",
    "- `ion_matches` (float): ion match rate between theoretical (Prosit-predicted) ions and experimental ions\n",
    "- `ion_match_intensity` (float): ion match intensity between theoretical (Prosit-predicted) ions and experimental ions\n",
    "- `iRT` (float):  Prosit-predicted iRT, based on the sequence value\n",
    "- `predicted iRT` (float): Winnow-predicted iRT, based on experimental retention time values\n",
    "- `iRT error` (float): absolute difference between `iRT` and `predicted iRT`\n",
    "- `runner_up_prosit_mz` (list(float)): Prosit-predicted m/z values for the runner up beam result\n",
    "- `runner_up_prosit_intensity` (list(float)): Prosit-predicted intensity values for the runner up beam result\n",
    "- `chimeric_ion_matches` (float): ion match rate between theoretical (Prosit-predicted) ions and experimental ions for the secondmost beam result\n",
    "- `chimeric_ion_match_intensity` (float): ion match intensity between theoretical (Prosit-predicted) ions and experimental ions for the secondmost beam result\n",
    "- `margin` (float): difference between probabilities of top and secondmost beam result\n",
    "- `median_margin` (float): difference between top beam result's probability and the median log-probability of the remaining runner-up beam results\n",
    "- `entropy` (float): Shannon entropy of normalised runner-up beam results\n",
    "- `z-score` (float): z-score of the sequence probabilities across the entire beam result for an MS2 spectrum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Calibrator evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A. Precision-recall curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- Plot precision-recall curve\n",
    "def compute_precision_recall_curve(\n",
    "    dataset: pd.DataFrame,\n",
    "    confidence_column: str,\n",
    "    label_column: str,\n",
    "    name: str,\n",
    ") -> pd.DataFrame:\n",
    "    original = dataset[[confidence_column, label_column]]\n",
    "    original = original.sort_values(by=confidence_column, ascending=False)\n",
    "    cum_correct = np.cumsum(original[label_column])\n",
    "    precision = cum_correct / np.arange(1, len(original) + 1)\n",
    "    recall = cum_correct / len(original)\n",
    "    metrics = pd.DataFrame({\"precision\": precision, \"recall\": recall}).reset_index(\n",
    "        drop=True\n",
    "    )\n",
    "    metrics[\"name\"] = name\n",
    "    return metrics\n",
    "\n",
    "\n",
    "original = compute_precision_recall_curve(\n",
    "    dataset=test_dataset.metadata,\n",
    "    confidence_column=\"confidence\",\n",
    "    label_column=\"correct\",\n",
    "    name=\"Original\",\n",
    ")\n",
    "calibrated = compute_precision_recall_curve(\n",
    "    dataset=test_dataset.metadata,\n",
    "    confidence_column=\"calibrated_confidence\",\n",
    "    label_column=\"correct\",\n",
    "    name=\"Calibrated\",\n",
    ")\n",
    "metrics = pd.concat([original, calibrated], axis=0).reset_index(drop=True)\n",
    "metrics[\"fdr\"] = 1 - metrics[\"precision\"]\n",
    "\n",
    "ax = sns.lineplot(data=metrics, x=\"recall\", y=\"precision\", hue=\"name\")\n",
    "ax.set(xlabel=\"Recall\", ylabel=\"Precision\", title=\"Precision-recall curve\")\n",
    "ax.legend(title=\"Confidence\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### B. Confidence score transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- Plot confidence score transformation\n",
    "data = test_dataset.metadata[[\"confidence\", \"calibrated_confidence\", \"correct\"]].copy(\n",
    "    deep=True\n",
    ")\n",
    "data = data.dropna(subset=[\"confidence\", \"calibrated_confidence\", \"correct\"])\n",
    "data[\"correct\"] = (\n",
    "    data[\"correct\"]\n",
    "    .map({True: \"True\", False: \"False\"})\n",
    "    .astype(pd.CategoricalDtype(categories=[\"True\", \"False\"], ordered=True))\n",
    ")\n",
    "\n",
    "ax = sns.scatterplot(\n",
    "    data=data,\n",
    "    x=\"confidence\",\n",
    "    y=\"calibrated_confidence\",\n",
    "    hue=\"correct\",\n",
    "    alpha=0.2,\n",
    "    legend=True,\n",
    ")\n",
    "ax.plot([0.0, 1.0], [0.0, 1.0], color=\"grey\", linestyle=\"--\")\n",
    "ax.set(\n",
    "    xlabel=\"Confidence\",\n",
    "    ylabel=\"Calibrated confidence\",\n",
    "    title=\"Confidence score transformation\",\n",
    ")\n",
    "\n",
    "legend = ax.get_legend()\n",
    "if legend is not None:\n",
    "    legend.set_title(\"Correct PSM\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### C. Confidence distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- Plot confidence distribution\n",
    "data = test_dataset.metadata[[\"confidence\", \"calibrated_confidence\", \"correct\"]].copy(\n",
    "    deep=True\n",
    ")\n",
    "data = data.dropna(subset=[\"confidence\", \"calibrated_confidence\", \"correct\"])\n",
    "data[\"correct\"] = (\n",
    "    data[\"correct\"]\n",
    "    .map({True: \"True\", False: \"False\"})\n",
    "    .astype(pd.CategoricalDtype(categories=[\"True\", \"False\"], ordered=True))\n",
    ")\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 5), sharey=True)\n",
    "\n",
    "# Raw confidence distribution\n",
    "ax0 = sns.histplot(\n",
    "    data=data,\n",
    "    x=\"confidence\",\n",
    "    hue=\"correct\",\n",
    "    hue_order=[\"True\", \"False\"],\n",
    "    bins=100,\n",
    "    multiple=\"layer\",\n",
    "    alpha=0.5,\n",
    "    legend=True,\n",
    "    ax=axes[0],\n",
    ")\n",
    "axes[0].set(xlabel=\"Confidence\", ylabel=\"Count\")\n",
    "sns.move_legend(axes[0], \"best\", title=\"Correct PSM\")\n",
    "\n",
    "# Calibrated confidence distribution\n",
    "ax1 = sns.histplot(\n",
    "    data=data,\n",
    "    x=\"calibrated_confidence\",\n",
    "    hue=\"correct\",\n",
    "    hue_order=[\"True\", \"False\"],\n",
    "    bins=100,\n",
    "    multiple=\"layer\",\n",
    "    alpha=0.5,\n",
    "    legend=True,\n",
    "    ax=axes[1],\n",
    ")\n",
    "axes[1].set(xlabel=\"Calibrated confidence\", ylabel=\"Count\")\n",
    "sns.move_legend(axes[1], \"best\", title=\"Correct PSM\")\n",
    "\n",
    "fig.suptitle(\"Confidence distribution\")\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### D. Calibration curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot calibration curve\n",
    "def compute_calibration_curve(\n",
    "    df: pd.DataFrame, pred_col: str, label_col: str, name: str, n_bins: int = 20\n",
    ") -> pd.DataFrame:\n",
    "    data = df[[pred_col, label_col]].dropna().copy(deep=True)\n",
    "    data[pred_col] = data[pred_col].clip(0.0, 1.0)\n",
    "    bins = np.linspace(0.0, 1.0, n_bins + 1)\n",
    "    bin_cats = pd.cut(data[pred_col], bins=bins, include_lowest=True)\n",
    "    bin_cats.name = \"bin\"\n",
    "    grouped = (\n",
    "        data.groupby(bin_cats, observed=True)\n",
    "        .agg(\n",
    "            pred_mean=(pred_col, \"mean\"),\n",
    "            empirical=(label_col, \"mean\"),\n",
    "            count=(label_col, \"size\"),\n",
    "        )\n",
    "        .reset_index()\n",
    "    )\n",
    "    grouped = grouped[grouped[\"count\"] > 0]\n",
    "    grouped[\"bin_center\"] = grouped[\"bin\"].apply(lambda iv: (iv.left + iv.right) / 2)\n",
    "    grouped[\"name\"] = name\n",
    "    return grouped[[\"pred_mean\", \"empirical\", \"count\", \"bin_center\", \"name\"]]\n",
    "\n",
    "\n",
    "metrics_orig = compute_calibration_curve(\n",
    "    test_dataset.metadata, \"confidence\", \"correct\", name=\"Original\", n_bins=10\n",
    ")\n",
    "metrics_cal = compute_calibration_curve(\n",
    "    test_dataset.metadata,\n",
    "    \"calibrated_confidence\",\n",
    "    \"correct\",\n",
    "    name=\"Calibrated\",\n",
    "    n_bins=10,\n",
    ")\n",
    "metrics = pd.concat([metrics_orig, metrics_cal], ignore_index=True)\n",
    "\n",
    "ax = sns.lineplot(data=metrics, x=\"pred_mean\", y=\"empirical\", hue=\"name\", marker=\"o\")\n",
    "ax.plot([0.0, 1.0], [0.0, 1.0], linestyle=\"--\", color=\"gray\", label=\"Ideal\")\n",
    "ax.set(\n",
    "    xlabel=\"Predicted probability\",\n",
    "    ylabel=\"Empirical probability\",\n",
    "    title=\"Calibration curve\",\n",
    ")\n",
    "ax.legend(title=\"Confidence\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## 7. FDR control: two approaches\n",
    "\n",
    "1. **Database-grounded FDR**\n",
    "  - Use when you have ground-truth peptide labels for this dataset (good for evaluation/benchmarking).\n",
    "  - Outputs: a confidence cutoff at a target FDR; optional per-PSM FDR via `add_psm_fdr` and q-values via `add_psm_q_value`.\n",
    "  - Extrapolates poorly to real *de novo* workflows\n",
    "\n",
    "2. **Non-parametric (label-free) FDR**\n",
    "  - Use on unlabelled datasets; uses calibrated confidence to directly estimate FDR (that is, the probability that a PSM is incorrect given its calibrated score is greater than or equal to a specified confidence score cutoff).\n",
    "  - No ground truth labels required\n",
    "  - Outputs: a confidence cutoff at a target FDR; per-PSM metrics: FDR (`psm_fdr`), q-value (`psm_q_value`), and posterior error probability (`psm_pep`).\n",
    "\n",
    "**Note**: Fit on the column you intend to use downstream (e.g., `calibrated_confidence`).\n",
    "\n",
    "### PSM-specific FDR metrics\n",
    "\n",
    "**Q-values and FDR**\n",
    "\n",
    "FDR for a PSM is the probability of that PSM being incorrect given that its confidence score is greater than or equal to a given cutoff $s$.  It is formally defined as: $P(\\text{incorrect} | S \\ge s)$. A q-value is the minimum FDR threshold at which a given PSM would be considered significant. More precisely, it's the smallest FDR cutoff that would still include this PSM in the filtered results.\n",
    "\n",
    "**Posterior error probability (PEP)**\n",
    "\n",
    "The PEP is the estimated probability that a given PSM is incorrect, given its observed features and confidence score. It is formally defined as: $P(\\text{incorrect} | \\text{observed features}, s)$. This metrics only considers the PSM at hand, whereas both PSM-specific FDR values and q-values can vary based on the surrounding PSMs in the analysis.\n",
    "\n",
    "**Example scenario**\n",
    "\n",
    "A PSM with confidence 0.85 might have a PSM-specific FDR of 0.02, a q-value of 0.015 , 0.04.\n",
    "- The PSM-specific FDR tells you: \"If I filter to confidence â‰¥ 0.85, I expect 2% of those PSMs to be wrong\"\n",
    "- The q-value tells you: \"This PSM would be significant even at a 1.5% FDR threshold\"\n",
    "- The PEP tells you: \"This specific PSM has a 4% chance of being wrong.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- Database-grounded FDR control\n",
    "database_grounded_fdr_control = DatabaseGroundedFDRControl(\n",
    "    confidence_feature=\"calibrated_confidence\"\n",
    ")\n",
    "database_grounded_fdr_control.fit(\n",
    "    dataset=test_dataset.metadata, residue_masses=RESIDUE_MASSES\n",
    ")\n",
    "print(\n",
    "    \"Database-grounded FDR control confidence cutoff at 5% FDR using calibrated confidence:\",\n",
    "    database_grounded_fdr_control.get_confidence_cutoff(threshold=0.05),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- Non-parametric FDR control (Winnow-specific)\n",
    "non_parametric_fdr_control = NonParametricFDRControl()\n",
    "non_parametric_fdr_control.fit(dataset=test_dataset.metadata[\"calibrated_confidence\"])\n",
    "print(\n",
    "    \"Non-parametric FDR control confidence cutoff at 5% FDR using calibrated confidence:\",\n",
    "    non_parametric_fdr_control.get_confidence_cutoff(threshold=0.05),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- Add PSM-specific FDR metrics to the dataset\n",
    "# Non-parametric FDR control\n",
    "test_dataset_winnow_metrics = non_parametric_fdr_control.add_psm_fdr(\n",
    "    test_dataset.metadata, confidence_col=\"calibrated_confidence\"\n",
    ")\n",
    "test_dataset_winnow_metrics = non_parametric_fdr_control.add_psm_q_value(\n",
    "    test_dataset_winnow_metrics, confidence_col=\"calibrated_confidence\"\n",
    ")\n",
    "test_dataset_winnow_metrics = non_parametric_fdr_control.add_psm_pep(\n",
    "    test_dataset_winnow_metrics, confidence_col=\"calibrated_confidence\"\n",
    ")\n",
    "\n",
    "# Database-grounded FDR control\n",
    "test_dataset_dbg_metrics = database_grounded_fdr_control.add_psm_fdr(\n",
    "    test_dataset.metadata, confidence_col=\"calibrated_confidence\"\n",
    ")\n",
    "test_dataset_dbg_metrics = database_grounded_fdr_control.add_psm_q_value(\n",
    "    test_dataset_dbg_metrics, confidence_col=\"calibrated_confidence\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = test_dataset_winnow_metrics[[\"calibrated_confidence\", \"psm_fdr\"]].copy(\n",
    "    deep=True\n",
    ")\n",
    "ax = sns.lineplot(\n",
    "    x=np.asarray(test_dataset_winnow_metrics[\"calibrated_confidence\"], dtype=float),\n",
    "    y=np.asarray(test_dataset_winnow_metrics[\"psm_fdr\"], dtype=float),\n",
    "    errorbar=None,\n",
    "    label=\"Winnow\",\n",
    ")\n",
    "ax = sns.lineplot(\n",
    "    x=np.asarray(test_dataset_dbg_metrics[\"calibrated_confidence\"], dtype=float),\n",
    "    y=np.asarray(test_dataset_dbg_metrics[\"psm_fdr\"], dtype=float),\n",
    "    errorbar=None,\n",
    "    label=\"Database-grounded\",\n",
    ")\n",
    "ax.set_xlabel(\"Calibrated confidence\")\n",
    "ax.set_ylabel(\"PSM FDR\")\n",
    "ax.set_title(\"PSM FDR vs. calibrated confidence\")\n",
    "ax.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Applying to new data\n",
    "\n",
    "This section simulates how you'd use winnow in a real *de novo* sequencing pipeline, when PSMs from other engines are not available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- Load the raw, unlabelled data\n",
    "logger.info(\"Loading raw dataset.\")\n",
    "dataset = CalibrationDataset.from_predictions_csv(\n",
    "    spectrum_path=\"winnow-ms-datasets/helaqc_raw_less_train.parquet\",\n",
    "    beam_predictions_path=\"winnow-ms-datasets/helaqc_raw_less_train_beams.csv\",\n",
    ")\n",
    "\n",
    "logger.info(\"Filtering dataset.\")\n",
    "raw_filtered_dataset = (\n",
    "    dataset.filter_entries(\n",
    "        metadata_predicate=lambda row: not isinstance(row[\"prediction\"], list),\n",
    "    )\n",
    "    .filter_entries(metadata_predicate=lambda row: not row[\"prediction\"])\n",
    "    .filter_entries(\n",
    "        metadata_predicate=lambda row: row[\"precursor_charge\"] > 6\n",
    "    )  # Prosit-specific filtering\n",
    "    .filter_entries(\n",
    "        metadata_predicate=lambda row: len(row[\"prediction\"]) > 30\n",
    "    )  # Prosit-specific filtering\n",
    "    .filter_entries(\n",
    "        predictions_predicate=lambda row: len(row[1].sequence) > 30\n",
    "    )  # Prosit-specific filtering\n",
    ")\n",
    "\n",
    "print(\"Number of PSMs in the unlabelled dataset:\", len(raw_filtered_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- Predict on the raw, unlabelled data\n",
    "calibrator.predict(raw_filtered_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- Non-parametric FDR control (Winnow-specific)\n",
    "non_parametric_fdr_control = NonParametricFDRControl()\n",
    "non_parametric_fdr_control.fit(\n",
    "    dataset=raw_filtered_dataset.metadata[\"calibrated_confidence\"]\n",
    ")\n",
    "confidence_cutoff = non_parametric_fdr_control.get_confidence_cutoff(threshold=0.05)\n",
    "print(\n",
    "    f\"Non-parametric FDR control confidence cutoff at 5% FDR using calibrated confidence: {confidence_cutoff}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- Add PSM-specific FDR metrics to the dataset\n",
    "raw_filtered_dataset_metadata = non_parametric_fdr_control.add_psm_fdr(\n",
    "    raw_filtered_dataset.metadata, confidence_col=\"calibrated_confidence\"\n",
    ")\n",
    "raw_filtered_dataset_metadata = non_parametric_fdr_control.add_psm_q_value(\n",
    "    raw_filtered_dataset_metadata, confidence_col=\"calibrated_confidence\"\n",
    ")\n",
    "raw_filtered_dataset_metadata = non_parametric_fdr_control.add_psm_pep(\n",
    "    raw_filtered_dataset_metadata, confidence_col=\"calibrated_confidence\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- Filter dataset to only include PSMs with calibrated confidence above the cutoff\n",
    "filtered_dataset = raw_filtered_dataset.filter_entries(\n",
    "    metadata_predicate=lambda row: row[\"calibrated_confidence\"] <= confidence_cutoff\n",
    ")\n",
    "\n",
    "print(f\"Number of PSMs in dataset at 5% FDR: {len(filtered_dataset)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Adding custom features\n",
    "\n",
    "How to implement your own calibration features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomFeature(CalibrationFeatures):\n",
    "    @property\n",
    "    def name(self) -> str:\n",
    "        return \"My Custom Feature\"\n",
    "\n",
    "    def compute(self, dataset: CalibrationDataset) -> None:\n",
    "        # Your feature computation logic\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Saving and reusing a calibrator\n",
    "\n",
    "If you plan to apply the same feature configuration to external data, you can save and reload the calibrator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- Save\n",
    "out_dir = Path(\"outputs/helaqc_calibrator\")\n",
    "ProbabilityCalibrator.save(calibrator, out_dir)\n",
    "\n",
    "# -- Load\n",
    "loaded = ProbabilityCalibrator.load(out_dir)\n",
    "print(\"Loaded features:\", loaded.features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. CLI usage\n",
    "\n",
    "You can run the same workflow headlessly via the CLI."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```bash\n",
    "# Train a calibrator\n",
    "winnow train \\\n",
    "  --data-source instanovo \\\n",
    "  --dataset-config-path configs/helaqc_labelled.yaml \\\n",
    "  --model-output-folder outputs/helaqc_calibrator \\\n",
    "  --dataset-output-path outputs/helaqc_train_outputs.csv\n",
    "\n",
    "# Predict + FDR on new data\n",
    "winnow predict \\\n",
    "  --data-source instanovo \\\n",
    "  --dataset-config-path configs/helaqc_raw.yaml \\\n",
    "  --model-folder outputs/helaqc_calibrator \\\n",
    "  --method winnow \\\n",
    "  --fdr-threshold 0.05 \\\n",
    "  --confidence-column calibrated_confidence \\\n",
    "  --output-path outputs/helaqc_filtered.csv\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Using Winnow without Prosit (offline/minimal)\n",
    "\n",
    "If you don't have Prosit access, have data with many diverse modifications, or want a minimal setup, you can disable Prosit-dependent features and still benefit from calibration using features available locally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Minimal feature set: no Prosit dependency\n",
    "cal_min = ProbabilityCalibrator(SEED)\n",
    "cal_min.add_feature(MassErrorFeature(residue_masses=RESIDUE_MASSES))\n",
    "cal_min.add_feature(BeamFeatures())\n",
    "\n",
    "cal_min.fit(train_dataset)\n",
    "cal_min.predict(test_dataset)\n",
    "print(\"Finished training minimal calibrator. Columns:\", cal_min.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. Pretrained general calibrator\n",
    "\n",
    "We have published a pretrained calibrator for InstaNovo predictions that uses the default feature set (Mass Error, Prosit spectral, Prosit iRT, Chimeric, Beam) on Hugging Face. You can load it and apply it to your own datasets without retraining!\n",
    "\n",
    "**Notes**:\n",
    "- Prosit-dependent features are still computed at inference, so Prosit access is required.\n",
    "- Ensure your modification tokens and input columns match this tutorial's format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- Download the general model\n",
    "calib_repo = \"InstaDeepAI/winnow-general-model\"\n",
    "calib_dir = Path(\"./winnow-general-model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "snapshot_download(repo_id=calib_repo, repo_type=\"model\", local_dir=calib_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- Load the general model\n",
    "general_model = ProbabilityCalibrator.load(calib_dir)\n",
    "print(\"Loaded pretrained features:\", general_model.features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's replicate results in our paper on the C. elegans dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- Download the C. elegans dataset\n",
    "snapshot_download(\n",
    "    repo_id=repo_id,\n",
    "    repo_type=\"dataset\",\n",
    "    allow_patterns=[\"celegans_labelled*.parquet\", \"celegans_labelled*.csv\"],\n",
    "    local_dir=save_dir,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- Load data\n",
    "logger.info(\"Loading dataset.\")\n",
    "celegans_dataset = CalibrationDataset.from_predictions_csv(\n",
    "    spectrum_path=\"winnow-ms-datasets/celegans_labelled.parquet\",\n",
    "    beam_predictions_path=\"winnow-ms-datasets/celegans_labelled_beams.csv\",\n",
    ")\n",
    "\n",
    "logger.info(\"Filtering dataset.\")\n",
    "celegans_filtered_dataset = (\n",
    "    celegans_dataset.filter_entries(\n",
    "        metadata_predicate=lambda row: not isinstance(row[\"prediction\"], list),\n",
    "    )\n",
    "    .filter_entries(metadata_predicate=lambda row: not row[\"prediction\"])\n",
    "    .filter_entries(\n",
    "        metadata_predicate=lambda row: row[\"precursor_charge\"] > 6\n",
    "    )  # Prosit-specific filtering, see https://github.com/Nesvilab/FragPipe/issues/1775\n",
    "    .filter_entries(\n",
    "        metadata_predicate=lambda row: len(row[\"prediction\"]) > 30\n",
    "    )  # Prosit-specific filtering\n",
    "    .filter_entries(\n",
    "        predictions_predicate=lambda row: len(row[1].sequence) > 30\n",
    "    )  # Prosit-specific filtering\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- Predict\n",
    "general_model.predict(celegans_filtered_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "original = compute_precision_recall_curve(\n",
    "    dataset=celegans_filtered_dataset.metadata,\n",
    "    confidence_column=\"confidence\",\n",
    "    label_column=\"correct\",\n",
    "    name=\"Original\",\n",
    ")\n",
    "calibrated = compute_precision_recall_curve(\n",
    "    dataset=celegans_filtered_dataset.metadata,\n",
    "    confidence_column=\"calibrated_confidence\",\n",
    "    label_column=\"correct\",\n",
    "    name=\"Calibrated\",\n",
    ")\n",
    "metrics = pd.concat([original, calibrated], axis=0).reset_index(drop=True)\n",
    "metrics[\"fdr\"] = 1 - metrics[\"precision\"]\n",
    "\n",
    "ax = sns.lineplot(data=metrics, x=\"recall\", y=\"precision\", hue=\"name\")\n",
    "ax.set(xlabel=\"Recall\", ylabel=\"Precision\", title=\"Precision-recall curve\")\n",
    "ax.legend(title=\"Confidence\", loc=\"lower left\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_orig = compute_calibration_curve(\n",
    "    celegans_filtered_dataset.metadata,\n",
    "    \"confidence\",\n",
    "    \"correct\",\n",
    "    name=\"Original\",\n",
    "    n_bins=10,\n",
    ")\n",
    "metrics_cal = compute_calibration_curve(\n",
    "    celegans_filtered_dataset.metadata,\n",
    "    \"calibrated_confidence\",\n",
    "    \"correct\",\n",
    "    name=\"Calibrated\",\n",
    "    n_bins=10,\n",
    ")\n",
    "metrics = pd.concat([metrics_orig, metrics_cal], ignore_index=True)\n",
    "\n",
    "ax = sns.lineplot(data=metrics, x=\"pred_mean\", y=\"empirical\", hue=\"name\", marker=\"o\")\n",
    "ax.plot([0.0, 1.0], [0.0, 1.0], linestyle=\"--\", color=\"gray\", label=\"Ideal\")\n",
    "ax.set(\n",
    "    xlabel=\"Predicted probability\",\n",
    "    ylabel=\"Empirical probability\",\n",
    "    title=\"Calibration curve\",\n",
    ")\n",
    "ax.legend(title=\"Confidence\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- Compute FDR\n",
    "# Non-parametric FDR control\n",
    "non_parametric_fdr_control = NonParametricFDRControl()\n",
    "non_parametric_fdr_control.fit(\n",
    "    dataset=celegans_filtered_dataset.metadata[\"calibrated_confidence\"]\n",
    ")\n",
    "confidence_cutoff = non_parametric_fdr_control.get_confidence_cutoff(threshold=0.05)\n",
    "print(\n",
    "    f\"Non-parametric FDR control confidence cutoff at 5% FDR using calibrated confidence: {confidence_cutoff}\"\n",
    ")\n",
    "\n",
    "# Database-grounded FDR control\n",
    "database_grounded_fdr_control = DatabaseGroundedFDRControl(\n",
    "    confidence_feature=\"calibrated_confidence\"\n",
    ")\n",
    "database_grounded_fdr_control.fit(\n",
    "    dataset=celegans_filtered_dataset.metadata, residue_masses=RESIDUE_MASSES\n",
    ")\n",
    "confidence_cutoff_dbg = database_grounded_fdr_control.get_confidence_cutoff(\n",
    "    threshold=0.05\n",
    ")\n",
    "print(\n",
    "    f\"Database-grounded FDR control confidence cutoff at 5% FDR using calibrated confidence: {confidence_cutoff_dbg}\"\n",
    ")\n",
    "\n",
    "# -- Calculate PSM-specific FDR metrics\n",
    "celegans_winnow_psm_fdr = non_parametric_fdr_control.add_psm_fdr(\n",
    "    celegans_filtered_dataset.metadata, confidence_col=\"calibrated_confidence\"\n",
    ")[[\"spectrum_id\", \"calibrated_confidence\", \"psm_fdr\"]]\n",
    "\n",
    "celegans_dbg_psm_fdr = database_grounded_fdr_control.add_psm_fdr(\n",
    "    celegans_filtered_dataset.metadata, confidence_col=\"calibrated_confidence\"\n",
    ")[[\"spectrum_id\", \"calibrated_confidence\", \"psm_fdr\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = celegans_winnow_psm_fdr[[\"calibrated_confidence\", \"psm_fdr\"]].copy(deep=True)\n",
    "ax = sns.lineplot(\n",
    "    x=np.asarray(celegans_winnow_psm_fdr[\"calibrated_confidence\"], dtype=float),\n",
    "    y=np.asarray(celegans_winnow_psm_fdr[\"psm_fdr\"], dtype=float),\n",
    "    errorbar=None,\n",
    "    label=\"Winnow\",\n",
    ")\n",
    "ax = sns.lineplot(\n",
    "    x=np.asarray(celegans_dbg_psm_fdr[\"calibrated_confidence\"], dtype=float),\n",
    "    y=np.asarray(celegans_dbg_psm_fdr[\"psm_fdr\"], dtype=float),\n",
    "    errorbar=None,\n",
    "    label=\"Database-grounded\",\n",
    ")\n",
    "ax.set_xlabel(\"Calibrated confidence\")\n",
    "ax.set_ylabel(\"PSM FDR\")\n",
    "ax.set_title(\"PSM FDR vs. calibrated confidence\")\n",
    "ax.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tips and troubleshooting\n",
    "- Ensure your spectrum data and beam predictions share a common, unique `spectrum_id` for merging.\n",
    "- Prosit-dependent features filter unsupported modifications; if you see many rows removed, review modification tokens.\n",
    "- If beam results often contain fewer than 2-3 candidates, chimeric/beam features may carry limited information (warnings will appear).\n",
    "- Always validate on a labelled subset or via consistency checks before deploying to new datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resources\n",
    "- ðŸ“š GitHub: https://github.com/instadeepai/winnow\n",
    "- ðŸ”§ CLI: `winnow --help`"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
