{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- Import\n",
    "from winnow.datasets.calibration_dataset import CalibrationDataset, RESIDUE_MASSES\n",
    "from winnow.datasets.data_loaders import InstaNovoDatasetLoader\n",
    "from winnow.scripts.main import (\n",
    "    filter_dataset,\n",
    "    initialise_calibrator,\n",
    ")\n",
    "from winnow.fdr.database_grounded import DatabaseGroundedFDRControl\n",
    "from winnow.fdr.nonparametric import NonParametricFDRControl\n",
    "\n",
    "import numpy as np\n",
    "import logging\n",
    "import os\n",
    "import glob\n",
    "import pandas as pd\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.calibration import calibration_curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- Set up logging\n",
    "logger = logging.getLogger(__name__)\n",
    "logger.setLevel(logging.INFO)\n",
    "logger.addHandler(logging.StreamHandler())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SPECIES = \"helaqc\"  # [gluc, helaqc, herceptin, immuno, sbrodae, snakevenoms, tplantibodies, woundfluids]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- Load data\n",
    "logger.info(\"Loading dataset.\")\n",
    "dataset = InstaNovoDatasetLoader().load(\n",
    "    \"../validation_datasets_corrected/spectrum_data/labelled/dataset-helaqc-annotated-0000-0001.parquet\",\n",
    "    \"../validation_datasets_corrected/beam_preds/labelled/helaqc-annotated_beam_preds.csv\",\n",
    ")\n",
    "\n",
    "logger.info(\"Filtering dataset.\")\n",
    "filtered_dataset = filter_dataset(dataset)\n",
    "\n",
    "# Split dataset\n",
    "TEST_FRACTION = 0.2\n",
    "RANDOM_STATE = 42\n",
    "train, test = train_test_split(\n",
    "    filtered_dataset, test_size=TEST_FRACTION, random_state=RANDOM_STATE\n",
    ")\n",
    "\n",
    "train_metadata, train_predictions = zip(*train)\n",
    "train_dataset = CalibrationDataset(\n",
    "    metadata=pd.DataFrame(train_metadata).reset_index(drop=True),\n",
    "    predictions=list(train_predictions),\n",
    ")\n",
    "\n",
    "test_metadata, test_predictions = zip(*test)\n",
    "test_dataset = CalibrationDataset(\n",
    "    metadata=pd.DataFrame(test_metadata).reset_index(drop=True),\n",
    "    predictions=list(test_predictions),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger.info(\"Training calibrator.\")\n",
    "calibrator = initialise_calibrator()\n",
    "calibrator.fit(train_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate model on labelled test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger.info(\"Calibrating scores.\")\n",
    "calibrator.predict(test_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger.info(\"Saving evaluation results.\")\n",
    "test_dataset.metadata.to_csv(f\"../{SPECIES}_results/test_dataset.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute FDR metrics on raw confidence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "non_parametric_fdr_control = NonParametricFDRControl()\n",
    "non_parametric_fdr_control.fit(dataset=test_dataset.metadata[\"confidence\"])\n",
    "\n",
    "logger.info(\n",
    "    f\"Winnow FDR threshold for raw confidence: {non_parametric_fdr_control.get_confidence_cutoff(threshold=0.05)}\"\n",
    ")\n",
    "\n",
    "test_dataset_metadata = non_parametric_fdr_control.add_psm_fdr(\n",
    "    test_dataset.metadata, \"confidence\"\n",
    ")\n",
    "test_dataset_metadata = non_parametric_fdr_control.add_psm_pep(\n",
    "    test_dataset_metadata, \"confidence\"\n",
    ")\n",
    "test_dataset_metadata = non_parametric_fdr_control.add_psm_qvalue(\n",
    "    test_dataset_metadata, \"confidence\"\n",
    ")\n",
    "\n",
    "# Save metrics\n",
    "test_dataset_metadata[[\"spectrum_id\", \"psm_fdr\", \"psm_pep\", \"psm_qvalue\"]].to_csv(\n",
    "    f\"../{SPECIES}_results/test_dataset_raw_confidence_winnow_fdr.csv\", index=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute FDR metrics on calibrated confidence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "non_parametric_fdr_control = NonParametricFDRControl()\n",
    "non_parametric_fdr_control.fit(dataset=test_dataset.metadata[\"calibrated_confidence\"])\n",
    "\n",
    "logger.info(\n",
    "    f\"Winnow FDR threshold for calibrated confidence: {non_parametric_fdr_control.get_confidence_cutoff(threshold=0.05)}\"\n",
    ")\n",
    "\n",
    "test_dataset_metadata = non_parametric_fdr_control.add_psm_fdr(\n",
    "    test_dataset.metadata, \"calibrated_confidence\"\n",
    ")\n",
    "test_dataset_metadata = non_parametric_fdr_control.add_psm_pep(\n",
    "    test_dataset_metadata, \"calibrated_confidence\"\n",
    ")\n",
    "test_dataset_metadata = non_parametric_fdr_control.add_psm_qvalue(\n",
    "    test_dataset_metadata, \"calibrated_confidence\"\n",
    ")\n",
    "\n",
    "# Save metrics\n",
    "test_dataset_metadata[[\"spectrum_id\", \"psm_fdr\", \"psm_pep\", \"psm_qvalue\"]].to_csv(\n",
    "    f\"../{SPECIES}_results/test_dataset_calibrated_confidence_winnow_fdr.csv\",\n",
    "    index=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute database-grounded FDR metrics on raw confidence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "database_grounded_fdr_control = DatabaseGroundedFDRControl(\n",
    "    confidence_feature=\"confidence\"\n",
    ")\n",
    "database_grounded_fdr_control.fit(\n",
    "    dataset=test_dataset.metadata, residue_masses=RESIDUE_MASSES\n",
    ")\n",
    "\n",
    "logger.info(\n",
    "    f\"Database-grounded FDR threshold for raw confidence: {database_grounded_fdr_control.get_confidence_cutoff(threshold=0.05)}\"\n",
    ")\n",
    "\n",
    "test_dataset_metadata = database_grounded_fdr_control.add_psm_fdr(\n",
    "    test_dataset.metadata, \"confidence\"\n",
    ")\n",
    "test_dataset_metadata = database_grounded_fdr_control.add_psm_qvalue(\n",
    "    test_dataset_metadata, \"confidence\"\n",
    ")\n",
    "\n",
    "# Save metrics\n",
    "test_dataset_metadata[[\"spectrum_id\", \"psm_fdr\", \"psm_qvalue\"]].to_csv(\n",
    "    f\"../{SPECIES}_results/test_dataset_raw_confidence_dbg_fdr.csv\", index=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute database-grounded FDR metrics on calibrated confidence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "database_grounded_fdr_control = DatabaseGroundedFDRControl(\n",
    "    confidence_feature=\"calibrated_confidence\"\n",
    ")\n",
    "database_grounded_fdr_control.fit(\n",
    "    dataset=test_dataset.metadata, residue_masses=RESIDUE_MASSES\n",
    ")\n",
    "\n",
    "logger.info(\n",
    "    f\"Database-grounded FDR threshold for calibrated confidence: {database_grounded_fdr_control.get_confidence_cutoff(threshold=0.05)}\"\n",
    ")\n",
    "\n",
    "test_dataset_metadata = database_grounded_fdr_control.add_psm_fdr(\n",
    "    test_dataset.metadata, \"calibrated_confidence\"\n",
    ")\n",
    "test_dataset_metadata = database_grounded_fdr_control.add_psm_qvalue(\n",
    "    test_dataset_metadata, \"calibrated_confidence\"\n",
    ")\n",
    "\n",
    "# Save metrics\n",
    "test_dataset_metadata[[\"spectrum_id\", \"psm_fdr\", \"psm_qvalue\"]].to_csv(\n",
    "    f\"../{SPECIES}_results/test_dataset_calibrated_confidence_dbg_fdr.csv\", index=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate model on full search space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- Load the raw, unlabelled data\n",
    "logger.info(\"Loading raw dataset.\")\n",
    "raw_dataset = InstaNovoDatasetLoader().load(\n",
    "    f\"../validation_datasets_corrected/spectrum_data/raw/{SPECIES}_raw_less_train.parquet\",\n",
    "    f\"../validation_datasets_corrected/beam_preds/raw/{SPECIES}_raw_less_train.csv\",\n",
    ")\n",
    "\n",
    "logger.info(\"Filtering dataset.\")\n",
    "raw_filtered_dataset = filter_dataset(raw_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger.info(\"Calibrating scores.\")\n",
    "calibrator.predict(raw_filtered_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger.info(\"Saving evaluation results.\")\n",
    "raw_filtered_dataset.metadata.to_csv(\n",
    "    f\"../{SPECIES}_results/raw_less_train.csv\", index=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute FDR metrics on raw confidence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "non_parametric_fdr_control = NonParametricFDRControl()\n",
    "non_parametric_fdr_control.fit(dataset=raw_filtered_dataset.metadata[\"confidence\"])\n",
    "\n",
    "logger.info(\n",
    "    f\"Winnow FDR threshold for raw confidence: {non_parametric_fdr_control.get_confidence_cutoff(threshold=0.05)}\"\n",
    ")\n",
    "\n",
    "raw_filtered_dataset_metadata = non_parametric_fdr_control.add_psm_fdr(\n",
    "    raw_filtered_dataset.metadata, \"confidence\"\n",
    ")\n",
    "raw_filtered_dataset_metadata = non_parametric_fdr_control.add_psm_pep(\n",
    "    raw_filtered_dataset_metadata, \"confidence\"\n",
    ")\n",
    "raw_filtered_dataset_metadata = non_parametric_fdr_control.add_psm_qvalue(\n",
    "    raw_filtered_dataset_metadata, \"confidence\"\n",
    ")\n",
    "\n",
    "# Save metrics\n",
    "raw_filtered_dataset_metadata[\n",
    "    [\"spectrum_id\", \"psm_fdr\", \"psm_pep\", \"psm_qvalue\"]\n",
    "].to_csv(\n",
    "    f\"../{SPECIES}_results/raw_less_train_dataset_raw_confidence_winnow_fdr.csv\",\n",
    "    index=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute FDR metrics on calibrated confidence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "non_parametric_fdr_control = NonParametricFDRControl()\n",
    "non_parametric_fdr_control.fit(\n",
    "    dataset=raw_filtered_dataset.metadata[\"calibrated_confidence\"]\n",
    ")\n",
    "\n",
    "logger.info(\n",
    "    f\"Winnow FDR threshold for calibrated confidence: {non_parametric_fdr_control.get_confidence_cutoff(threshold=0.05)}\"\n",
    ")\n",
    "\n",
    "raw_filtered_dataset_metadata = non_parametric_fdr_control.add_psm_fdr(\n",
    "    raw_filtered_dataset.metadata, \"calibrated_confidence\"\n",
    ")\n",
    "raw_filtered_dataset_metadata = non_parametric_fdr_control.add_psm_pep(\n",
    "    raw_filtered_dataset_metadata, \"calibrated_confidence\"\n",
    ")\n",
    "raw_filtered_dataset_metadata = non_parametric_fdr_control.add_psm_qvalue(\n",
    "    raw_filtered_dataset_metadata, \"calibrated_confidence\"\n",
    ")\n",
    "\n",
    "# Save metrics\n",
    "raw_filtered_dataset_metadata[\n",
    "    [\"spectrum_id\", \"psm_fdr\", \"psm_pep\", \"psm_qvalue\"]\n",
    "].to_csv(\n",
    "    f\"../{SPECIES}_results/raw_less_train_dataset_calibrated_confidence_winnow_fdr.csv\",\n",
    "    index=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate model on de novo data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- Load the raw, unlabelled data\n",
    "logger.info(\"Loading de novo dataset.\")\n",
    "raw_dataset = InstaNovoDatasetLoader().load(\n",
    "    f\"../validation_datasets_corrected/spectrum_data/de_novo/{SPECIES}_raw_filtered.parquet\",\n",
    "    f\"../validation_datasets_corrected/beam_preds/de_novo/{SPECIES}_raw_beam_preds_filtered.csv\",\n",
    ")\n",
    "\n",
    "logger.info(\"Filtering dataset.\")\n",
    "raw_filtered_dataset = filter_dataset(raw_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger.info(\"Calibrating scores.\")\n",
    "calibrator.predict(raw_filtered_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger.info(\"Saving evaluation results.\")\n",
    "raw_filtered_dataset.metadata.to_csv(\n",
    "    f\"../{SPECIES}_results/raw_filtered_dataset.csv\", index=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute FDR metrics on raw confidence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "non_parametric_fdr_control = NonParametricFDRControl()\n",
    "non_parametric_fdr_control.fit(dataset=raw_filtered_dataset.metadata[\"confidence\"])\n",
    "\n",
    "logger.info(\n",
    "    f\"Winnow FDR threshold for raw confidence: {non_parametric_fdr_control.get_confidence_cutoff(threshold=0.05)}\"\n",
    ")\n",
    "\n",
    "raw_filtered_dataset_metadata = non_parametric_fdr_control.add_psm_fdr(\n",
    "    raw_filtered_dataset.metadata, \"confidence\"\n",
    ")\n",
    "raw_filtered_dataset_metadata = non_parametric_fdr_control.add_psm_pep(\n",
    "    raw_filtered_dataset_metadata, \"confidence\"\n",
    ")\n",
    "raw_filtered_dataset_metadata = non_parametric_fdr_control.add_psm_qvalue(\n",
    "    raw_filtered_dataset_metadata, \"confidence\"\n",
    ")\n",
    "\n",
    "# Save metrics\n",
    "raw_filtered_dataset_metadata[\n",
    "    [\"spectrum_id\", \"psm_fdr\", \"psm_pep\", \"psm_qvalue\"]\n",
    "].to_csv(\n",
    "    f\"../{SPECIES}_results/de_novo_dataset_raw_confidence_winnow_fdr.csv\", index=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute FDR metrics on calibrated confidence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "non_parametric_fdr_control = NonParametricFDRControl()\n",
    "non_parametric_fdr_control.fit(\n",
    "    dataset=raw_filtered_dataset.metadata[\"calibrated_confidence\"]\n",
    ")\n",
    "\n",
    "logger.info(\n",
    "    f\"Winnow FDR threshold for calibrated confidence: {non_parametric_fdr_control.get_confidence_cutoff(threshold=0.05)}\"\n",
    ")\n",
    "\n",
    "raw_filtered_dataset_metadata = non_parametric_fdr_control.add_psm_fdr(\n",
    "    raw_filtered_dataset.metadata, \"calibrated_confidence\"\n",
    ")\n",
    "raw_filtered_dataset_metadata = non_parametric_fdr_control.add_psm_pep(\n",
    "    raw_filtered_dataset_metadata, \"calibrated_confidence\"\n",
    ")\n",
    "raw_filtered_dataset_metadata = non_parametric_fdr_control.add_psm_qvalue(\n",
    "    raw_filtered_dataset_metadata, \"calibrated_confidence\"\n",
    ")\n",
    "\n",
    "# Save metrics\n",
    "raw_filtered_dataset_metadata[\n",
    "    [\"spectrum_id\", \"psm_fdr\", \"psm_pep\", \"psm_qvalue\"]\n",
    "].to_csv(\n",
    "    f\"../{SPECIES}_results/de_novo_dataset_calibrated_confidence_winnow_fdr.csv\",\n",
    "    index=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot proteome mapping results\n",
    "\n",
    "We want to illustrate using proteome mapping in this panel, using the helaqc dataset.\n",
    "\n",
    "- PR plot (confidence vs calibrated confidence)\n",
    "- FDR run plot (DBG FDR on confidence and on calibrated confidence; winnow on calibrated confidence)\n",
    "- Calibration plot (confidence vs calibrated confidence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Color scheme from the attached notebook\n",
    "COLORS = {\n",
    "    \"fairy\": \"#FFCAE9\",\n",
    "    \"magenta\": \"#8E5572\",\n",
    "    \"ash\": \"#BBC5AA\",\n",
    "    \"ebony\": \"#5A6650\",\n",
    "    \"sky\": \"#7FC8F8\",\n",
    "    \"navy\": \"#3C81AE\",\n",
    "}\n",
    "\n",
    "# Species name mapping for nicer plot labels\n",
    "SPECIES_NAME_MAPPING = {\n",
    "    \"gluc\": \"HeLa degradome\",\n",
    "    \"helaqc\": \"HeLa single shot\",\n",
    "    \"herceptin\": \"Herceptin\",\n",
    "    \"immuno\": \"Immunopeptidomics-1\",\n",
    "    \"sbrodae\": \"Scalindua brodae\",\n",
    "    \"snakevenoms\": \"Snake venomics\",\n",
    "    \"woundfluids\": \"Wound exudates\",\n",
    "    \"PXD014877\": \"C. elegans\",\n",
    "    \"PXD019483\": \"HepG2\",\n",
    "    \"PXD023064\": \"Immunopeptidomics-2\",\n",
    "    \"general\": \"General test set\",\n",
    "}\n",
    "\n",
    "# Configure matplotlib to match seaborn \"paper\" context with font_scale=2\n",
    "plt.style.use(\"seaborn-v0_8-paper\")\n",
    "plt.rcParams.update(\n",
    "    {\n",
    "        # Figure settings\n",
    "        \"figure.figsize\": [12.0, 10.0],\n",
    "        \"figure.facecolor\": \"white\",\n",
    "        \"figure.dpi\": 100.0,\n",
    "        # Axes settings\n",
    "        \"axes.labelcolor\": \".15\",\n",
    "        \"axes.axisbelow\": True,\n",
    "        \"axes.grid\": False,\n",
    "        \"axes.facecolor\": \"white\",\n",
    "        \"axes.edgecolor\": \".15\",\n",
    "        \"axes.linewidth\": 1.0,\n",
    "        \"axes.spines.left\": True,\n",
    "        \"axes.spines.bottom\": True,\n",
    "        \"axes.spines.right\": True,\n",
    "        \"axes.spines.top\": True,\n",
    "        # Tick settings\n",
    "        \"xtick.direction\": \"out\",\n",
    "        \"ytick.direction\": \"out\",\n",
    "        \"xtick.color\": \".15\",\n",
    "        \"ytick.color\": \".15\",\n",
    "        \"xtick.top\": False,\n",
    "        \"ytick.right\": False,\n",
    "        \"xtick.bottom\": False,\n",
    "        \"ytick.left\": False,\n",
    "        \"xtick.major.width\": 1.0,\n",
    "        \"ytick.major.width\": 1.0,\n",
    "        \"xtick.minor.width\": 0.8,\n",
    "        \"ytick.minor.width\": 0.8,\n",
    "        \"xtick.major.size\": 4.8,\n",
    "        \"ytick.major.size\": 4.8,\n",
    "        \"xtick.minor.size\": 3.2,\n",
    "        \"ytick.minor.size\": 3.2,\n",
    "        # Grid settings\n",
    "        \"grid.linestyle\": \"-\",\n",
    "        \"grid.color\": \".8\",\n",
    "        \"grid.linewidth\": 0.8,\n",
    "        # Text settings\n",
    "        \"text.color\": \".15\",\n",
    "        \"font.family\": [\"sans-serif\"],\n",
    "        \"font.sans-serif\": [\n",
    "            \"Arial\",\n",
    "            \"DejaVu Sans\",\n",
    "            \"Liberation Sans\",\n",
    "            \"Bitstream Vera Sans\",\n",
    "            \"sans-serif\",\n",
    "        ],\n",
    "        \"font.size\": 9.6,\n",
    "        \"axes.labelsize\": 9.6,\n",
    "        \"axes.titlesize\": 9.6,\n",
    "        \"xtick.labelsize\": 8.8,\n",
    "        \"ytick.labelsize\": 8.8,\n",
    "        \"legend.fontsize\": 8.8,\n",
    "        \"legend.title_fontsize\": 9.6,\n",
    "        # Line and patch settings\n",
    "        \"lines.linewidth\": 1.2,\n",
    "        \"lines.markersize\": 4.8,\n",
    "        \"lines.solid_capstyle\": \"round\",\n",
    "        \"patch.linewidth\": 0.8,\n",
    "        \"patch.edgecolor\": \"black\",\n",
    "        \"patch.force_edgecolor\": True,\n",
    "        # Image settings\n",
    "        \"image.cmap\": \"rocket\",\n",
    "    }\n",
    ")\n",
    "# paper_params = {\n",
    "#     # bar edge settings\n",
    "#     \"patch.force_edgecolor\": True,   # force edgecolors on histogram/bar patches\n",
    "#     \"patch.edgecolor\": \"black\",      # default edge color for patches\n",
    "#     \"patch.linewidth\": 1.0,          # border thickness\n",
    "# }\n",
    "# mpl.rcParams.update(paper_params)\n",
    "\n",
    "# Print style information\n",
    "print(\"Matplotlib seaborn-v0_0-paper Style Characteristics:\")\n",
    "print(f\"Figure size: {plt.rcParams['figure.figsize']}\")\n",
    "print(f\"DPI: {plt.rcParams['figure.dpi']}\")\n",
    "print(f\"Font size: {plt.rcParams['font.size']}\")\n",
    "print(f\"Axes line width: {plt.rcParams['axes.linewidth']}\")\n",
    "\n",
    "print(\"\\nKey rcParams settings:\")\n",
    "paper_relevant_params = [\n",
    "    \"figure.figsize\",\n",
    "    \"figure.dpi\",\n",
    "    \"font.size\",\n",
    "    \"axes.linewidth\",\n",
    "    \"axes.grid\",\n",
    "    \"axes.spines.left\",\n",
    "    \"axes.spines.bottom\",\n",
    "    \"axes.spines.top\",\n",
    "    \"axes.spines.right\",\n",
    "    \"xtick.bottom\",\n",
    "    \"xtick.top\",\n",
    "    \"ytick.left\",\n",
    "    \"ytick.right\",\n",
    "    \"axes.axisbelow\",\n",
    "    \"grid.linewidth\",\n",
    "    \"lines.linewidth\",\n",
    "    \"patch.linewidth\",\n",
    "    \"lines.markersize\",\n",
    "    \"axes.titlesize\",\n",
    "    \"axes.labelsize\",\n",
    "    \"xtick.labelsize\",\n",
    "    \"ytick.labelsize\",\n",
    "    \"legend.fontsize\",\n",
    "]\n",
    "\n",
    "for param in paper_relevant_params:\n",
    "    if param in plt.rcParams:\n",
    "        print(f\"  {param}: {plt.rcParams[param]}\")\n",
    "\n",
    "print(f\"\\nCurrent style: {mpl.get_backend()}\")\n",
    "print(\n",
    "    f\"Available styles containing 'seaborn': {[s for s in plt.style.available if 'seaborn' in s]}\"\n",
    ")\n",
    "\n",
    "\n",
    "def compute_pr_curve(\n",
    "    input_dataset: pd.DataFrame,\n",
    "    confidence_column: str,\n",
    "    label_column: str,\n",
    "    name: str,\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"Compute precision-recall curve for given confidence scores and labels.\n",
    "\n",
    "    Args:\n",
    "        input_dataset: DataFrame containing confidence scores and labels\n",
    "        confidence_column: Name of the column containing confidence scores\n",
    "        label_column: Name of the column containing boolean labels\n",
    "        name: Name to assign to the computed curve\n",
    "\n",
    "    Returns:\n",
    "        DataFrame with precision, recall, and name columns\n",
    "    \"\"\"\n",
    "    original = input_dataset[[confidence_column, label_column]]\n",
    "    original = original.sort_values(by=confidence_column, ascending=False)\n",
    "    cum_correct = np.cumsum(original[label_column])\n",
    "    precision = cum_correct / np.arange(1, len(original) + 1)\n",
    "    recall = cum_correct / len(original)\n",
    "    metrics = pd.DataFrame({\"precision\": precision, \"recall\": recall}).reset_index(\n",
    "        drop=True\n",
    "    )\n",
    "    metrics[\"name\"] = name\n",
    "    return metrics\n",
    "\n",
    "\n",
    "def plot_pr_curve_on_axes(\n",
    "    metadata: pd.DataFrame,\n",
    "    ax: plt.Axes,\n",
    "    title: str = \"Precision-Recall Curve\",\n",
    "    label_column: str = \"correct\",\n",
    ") -> None:\n",
    "    \"\"\"Plot precision-recall curves for original and calibrated confidence on a given axes.\"\"\"\n",
    "    # Compute PR curves\n",
    "    original = compute_pr_curve(\n",
    "        input_dataset=metadata,\n",
    "        confidence_column=\"confidence\",\n",
    "        label_column=label_column,\n",
    "        name=\"Raw confidence\",\n",
    "    )\n",
    "    calibrated = compute_pr_curve(\n",
    "        input_dataset=metadata,\n",
    "        confidence_column=\"calibrated_confidence\",\n",
    "        label_column=label_column,\n",
    "        name=\"Calibrated confidence\",\n",
    "    )\n",
    "    metrics = pd.concat([original, calibrated], axis=0).reset_index(drop=True)\n",
    "\n",
    "    # Plot each curve with new color scheme\n",
    "    for name, group in metrics.groupby(\"name\"):\n",
    "        if name == \"Raw confidence\":\n",
    "            color = COLORS[\"sky\"]  # Sky blue for original\n",
    "        else:\n",
    "            color = COLORS[\"ebony\"]  # Ebony for calibrated\n",
    "        ax.plot(group[\"recall\"], group[\"precision\"], label=name, color=color, zorder=2)\n",
    "\n",
    "    ax.set_axisbelow(True)\n",
    "    ax.grid(True, color=\"lightgray\", zorder=0)\n",
    "    ax.set_xlabel(\"Recall\")\n",
    "    ax.set_ylabel(\"Precision\")\n",
    "    ax.set_title(title)\n",
    "    ax.legend()\n",
    "    ax.set_xlim(0, 1)\n",
    "    ax.set_ylim(0, 1)\n",
    "\n",
    "\n",
    "def plot_confidence_distribution_on_axes(\n",
    "    metadata: pd.DataFrame,\n",
    "    ax: plt.Axes,\n",
    "    confidence_column: str = \"confidence\",\n",
    "    title: str = \"Confidence Distribution\",\n",
    "    density: bool = False,\n",
    "    label_column: str = \"correct\",\n",
    ") -> None:\n",
    "    \"\"\"Plot confidence distribution on a given axes.\"\"\"\n",
    "    plot_df = metadata[[confidence_column, label_column]].copy(deep=True)\n",
    "    plot_df[label_column] = plot_df[label_column].apply(lambda x: \"T\" if x else \"F\")\n",
    "\n",
    "    true_conf = plot_df[plot_df[label_column] == \"T\"][confidence_column]\n",
    "    false_conf = plot_df[plot_df[label_column] == \"F\"][confidence_column]\n",
    "\n",
    "    ax.hist(\n",
    "        false_conf,\n",
    "        bins=50,\n",
    "        alpha=0.7,\n",
    "        label=\"Incorrect\",\n",
    "        color=COLORS[\"sky\"],\n",
    "        density=density,\n",
    "        edgecolor=\"#333333\",\n",
    "    )\n",
    "    ax.hist(\n",
    "        true_conf,\n",
    "        bins=50,\n",
    "        alpha=0.7,\n",
    "        label=\"Correct\",\n",
    "        color=COLORS[\"ebony\"],\n",
    "        density=density,\n",
    "        edgecolor=\"#333333\",\n",
    "    )\n",
    "    ax.set_xlabel(confidence_column.replace(\"_\", \" \").title())\n",
    "    if density:\n",
    "        ax.set_ylabel(\"Density\")\n",
    "    else:\n",
    "        ax.set_ylabel(\"Frequency\")\n",
    "    ax.set_title(title)\n",
    "    ax.legend()\n",
    "\n",
    "\n",
    "def plot_calibration_curve_on_axes(\n",
    "    metadata: pd.DataFrame,\n",
    "    ax: plt.Axes,\n",
    "    confidence_column: str = \"confidence\",\n",
    "    title: str = \"Confidence Calibration\",\n",
    "    label_column: str = \"correct\",\n",
    ") -> None:\n",
    "    \"\"\"Plot probability calibration curve on a given axes.\"\"\"\n",
    "    confidence_scores = metadata[confidence_column].values\n",
    "    true_labels = metadata[label_column].values\n",
    "\n",
    "    # Calculate calibration curve\n",
    "    fraction_of_positives, mean_predicted_value = calibration_curve(\n",
    "        true_labels, confidence_scores, n_bins=10, strategy=\"uniform\"\n",
    "    )\n",
    "\n",
    "    # Determine color based on confidence column\n",
    "    if confidence_column == \"confidence\":\n",
    "        color = COLORS[\"sky\"]  # Sky for original\n",
    "        label = \"Raw confidence\"\n",
    "    else:\n",
    "        color = COLORS[\"ebony\"]  # Ebony for calibrated\n",
    "        label = \"Calibrated confidence\"\n",
    "\n",
    "    # Plot calibration curve\n",
    "    ax.plot(\n",
    "        mean_predicted_value,\n",
    "        fraction_of_positives,\n",
    "        \"s-\",\n",
    "        label=label,\n",
    "        color=color,\n",
    "        zorder=2,\n",
    "    )\n",
    "    ax.plot([0, 1], [0, 1], \"k--\", label=\"Perfectly calibrated\", alpha=0.5, zorder=2)\n",
    "    ax.set_axisbelow(True)\n",
    "    ax.grid(True, color=\"lightgray\", zorder=0)\n",
    "    ax.set_xlabel(\"Mean predicted probability\")\n",
    "    ax.set_ylabel(\"Fraction of positives\")\n",
    "    ax.set_title(title)\n",
    "    ax.legend()\n",
    "    ax.set_xlim([0, 1])\n",
    "    ax.set_ylim([0, 1])\n",
    "\n",
    "\n",
    "def plot_combined_calibration_curves(\n",
    "    metadata: pd.DataFrame,\n",
    "    title: str = \"Confidence Calibration Comparison\",\n",
    "    label_column: str = \"correct\",\n",
    ") -> plt.Figure:\n",
    "    \"\"\"Plot both original and calibrated confidence calibration curves on a single axis.\"\"\"\n",
    "    fig, ax = plt.subplots(figsize=(8, 6))\n",
    "\n",
    "    # Plot original confidence calibration\n",
    "    confidence_scores = metadata[\"confidence\"].values\n",
    "    true_labels = metadata[label_column].values\n",
    "    fraction_of_positives, mean_predicted_value = calibration_curve(\n",
    "        true_labels, confidence_scores, n_bins=10, strategy=\"uniform\"\n",
    "    )\n",
    "    ax.plot(\n",
    "        mean_predicted_value,\n",
    "        fraction_of_positives,\n",
    "        \"s-\",\n",
    "        label=\"Raw confidence\",\n",
    "        color=COLORS[\"sky\"],\n",
    "        zorder=2,\n",
    "    )\n",
    "\n",
    "    # Plot calibrated confidence calibration\n",
    "    calibrated_scores = metadata[\"calibrated_confidence\"].values\n",
    "    fraction_of_positives_cal, mean_predicted_value_cal = calibration_curve(\n",
    "        true_labels, calibrated_scores, n_bins=10, strategy=\"uniform\"\n",
    "    )\n",
    "    ax.plot(\n",
    "        mean_predicted_value_cal,\n",
    "        fraction_of_positives_cal,\n",
    "        \"o-\",\n",
    "        label=\"Calibrated confidence\",\n",
    "        color=COLORS[\"ebony\"],\n",
    "        zorder=2,\n",
    "    )\n",
    "\n",
    "    # Perfect calibration line\n",
    "    ax.plot([0, 1], [0, 1], \"k--\", label=\"Perfectly calibrated\", alpha=0.5, zorder=2)\n",
    "\n",
    "    ax.set_axisbelow(True)\n",
    "    ax.grid(True, color=\"lightgray\", zorder=0)\n",
    "\n",
    "    ax.set_xlabel(\"Mean predicted probability\")\n",
    "    ax.set_ylabel(\"Fraction of positives\")\n",
    "    ax.set_title(title)\n",
    "    ax.legend()\n",
    "    ax.set_xlim([0, 1])\n",
    "    ax.set_ylim([0, 1])\n",
    "\n",
    "    return fig\n",
    "\n",
    "\n",
    "def plot_combined_confidence_distributions(\n",
    "    metadata: pd.DataFrame,\n",
    "    title: str = \"Confidence Distributions\",\n",
    "    label_column: str = \"correct\",\n",
    ") -> plt.Figure:\n",
    "    \"\"\"Plot both original and calibrated confidence distributions on a single panel.\"\"\"\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "    # Original confidence distribution\n",
    "    plot_confidence_distribution_on_axes(\n",
    "        metadata,\n",
    "        ax1,\n",
    "        \"confidence\",\n",
    "        \"Raw confidence distribution\",\n",
    "        label_column=label_column,\n",
    "    )\n",
    "\n",
    "    # Calibrated confidence distribution\n",
    "    plot_confidence_distribution_on_axes(\n",
    "        metadata,\n",
    "        ax2,\n",
    "        \"calibrated_confidence\",\n",
    "        \"Calibrated confidence distribution\",\n",
    "        label_column=label_column,\n",
    "    )\n",
    "\n",
    "    plt.suptitle(title)\n",
    "    return fig\n",
    "\n",
    "\n",
    "def get_plot_dataframe(\n",
    "    features_df: pd.DataFrame,\n",
    "    winnow_metrics_df: pd.DataFrame,\n",
    "    decoy_metrics_df: pd.DataFrame,\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"Create a dataframe for FDR plotting with true and estimated FDR values.\n",
    "\n",
    "    Args:\n",
    "        features_df: DataFrame containing features\n",
    "        winnow_metrics_df: DataFrame containing Winnow metrics\n",
    "        decoy_metrics_df: DataFrame containing decoy metrics\n",
    "\n",
    "    Returns:\n",
    "        DataFrame with confidence, FDR, and source columns\n",
    "    \"\"\"\n",
    "    metrics_df = pd.merge(\n",
    "        decoy_metrics_df,\n",
    "        winnow_metrics_df,\n",
    "        on=\"spectrum_id\",\n",
    "        how=\"inner\",\n",
    "        suffixes=(\"_dbg\", \"_winnow\"),\n",
    "    )\n",
    "    df = pd.merge(\n",
    "        features_df[[\"spectrum_id\", \"calibrated_confidence\"]],\n",
    "        metrics_df,\n",
    "        on=\"spectrum_id\",\n",
    "        how=\"inner\",\n",
    "    ).sort_values(by=\"calibrated_confidence\")\n",
    "    return df\n",
    "\n",
    "\n",
    "def plot_fdr_accuracy_on_axes(\n",
    "    metadata: pd.DataFrame,\n",
    "    winnow_metrics_df: pd.DataFrame,\n",
    "    decoy_metrics_df: pd.DataFrame,\n",
    "    ax: plt.Axes,\n",
    "    title: str = \"FDR Accuracy\",\n",
    ") -> None:\n",
    "    \"\"\"Plot FDR accuracy comparison on a given axes.\n",
    "\n",
    "    Args:\n",
    "        metadata: DataFrame containing confidence scores and labels\n",
    "        ax: Matplotlib axes to plot on\n",
    "        fdr_function: Function to calculate estimated FDR\n",
    "        confidence_column: Name of the column containing confidence scores\n",
    "        title: Title for the plot\n",
    "        label_column: Name of the column containing boolean labels\n",
    "    \"\"\"\n",
    "    # Get the multi-plot dataframe\n",
    "    multi_plot_df = get_plot_dataframe(\n",
    "        features_df=metadata,\n",
    "        winnow_metrics_df=winnow_metrics_df,\n",
    "        decoy_metrics_df=decoy_metrics_df,\n",
    "    )\n",
    "\n",
    "    # Plot FDR lines for each source\n",
    "    ax.plot(\n",
    "        multi_plot_df[\"calibrated_confidence\"],\n",
    "        multi_plot_df[\"psm_fdr_winnow\"],\n",
    "        label=\"Winnow FDR\",\n",
    "        color=COLORS[\"sky\"],\n",
    "        zorder=2,\n",
    "    )\n",
    "    ax.plot(\n",
    "        multi_plot_df[\"calibrated_confidence\"],\n",
    "        multi_plot_df[\"psm_fdr_dbg\"],\n",
    "        label=\"Decoy FDR\",\n",
    "        color=COLORS[\"ebony\"],\n",
    "        zorder=2,\n",
    "    )\n",
    "\n",
    "    # Add horizontal line at FDR = 0.05\n",
    "    ax.axhline(y=0.05, color=\"black\", linestyle=\"--\", alpha=0.5, zorder=2)\n",
    "\n",
    "    # Customize the plot\n",
    "    ax.set_axisbelow(True)\n",
    "    ax.grid(True, color=\"lightgray\", zorder=0)\n",
    "    ax.set_xlabel(\"Calibrated Confidence\")\n",
    "    ax.set_ylabel(\"False discovery rate (FDR)\")\n",
    "    ax.set_title(title)\n",
    "    ax.legend()\n",
    "\n",
    "\n",
    "def create_fdr_accuracy_plot(\n",
    "    metadata: pd.DataFrame,\n",
    "    winnow_metrics_df: pd.DataFrame,\n",
    "    decoy_metrics_df: pd.DataFrame,\n",
    "    title: str = \"FDR Accuracy\",\n",
    ") -> plt.Figure:\n",
    "    \"\"\"Create standalone FDR accuracy plot.\"\"\"\n",
    "    fig, ax = plt.subplots(figsize=(8, 6))\n",
    "    plot_fdr_accuracy_on_axes(\n",
    "        metadata,\n",
    "        winnow_metrics_df,\n",
    "        decoy_metrics_df,\n",
    "        ax,\n",
    "        title,\n",
    "    )\n",
    "    return fig\n",
    "\n",
    "\n",
    "def create_pr_curve_plot(\n",
    "    metadata: pd.DataFrame,\n",
    "    title: str = \"Precision-Recall Curve\",\n",
    "    label_column: str = \"correct\",\n",
    ") -> plt.Figure:\n",
    "    \"\"\"Create standalone precision-recall curve plot.\"\"\"\n",
    "    fig, ax = plt.subplots(figsize=(8, 6))\n",
    "    plot_pr_curve_on_axes(metadata, ax, title, label_column)\n",
    "    return fig\n",
    "\n",
    "\n",
    "def find_data_files(base_dir: str = \"new_model/results\") -> dict:\n",
    "    \"\"\"Find all relevant data files in the results directory.\n",
    "\n",
    "    Args:\n",
    "        base_dir: Base directory to search for files\n",
    "\n",
    "    Returns:\n",
    "        Dictionary with file categories and their paths\n",
    "    \"\"\"\n",
    "    files: dict[str, list[str]] = {\"labelled\": [], \"de_novo\": [], \"raw\": []}\n",
    "\n",
    "    # Find all CSV files in the directory\n",
    "    csv_files = []\n",
    "    for pattern in [\"*.csv\", \"*.csv.*\"]:  # Include files with suffixes\n",
    "        csv_files.extend(glob.glob(os.path.join(base_dir, pattern)))\n",
    "\n",
    "    for file_path in csv_files:\n",
    "        file_name = os.path.basename(file_path)\n",
    "\n",
    "        if file_name.startswith(\"labelled_\"):\n",
    "            files[\"labelled\"].append(file_path)\n",
    "        elif file_name.startswith(\"de_novo_\"):\n",
    "            files[\"de_novo\"].append(file_path)\n",
    "        elif file_name.startswith(\"raw_\"):\n",
    "            files[\"raw\"].append(file_path)\n",
    "\n",
    "    return files\n",
    "\n",
    "\n",
    "def extract_dataset_name(file_path: str) -> str:\n",
    "    \"\"\"Extract dataset name from file path.\n",
    "\n",
    "    Args:\n",
    "        file_path: Path to the data file\n",
    "\n",
    "    Returns:\n",
    "        Dataset name\n",
    "    \"\"\"\n",
    "    file_name = os.path.basename(file_path)\n",
    "\n",
    "    if file_name.startswith(\"labelled_\"):\n",
    "        # Remove \"labelled_\" prefix and \".csv\" suffix (and any additional suffixes)\n",
    "        name = file_name[9:]  # Remove \"labelled_\"\n",
    "        name = name.split(\".csv\")[0]  # Remove .csv and any suffixes\n",
    "        return name.replace(\"_results\", \"\")\n",
    "    elif file_name.startswith(\"de_novo_\"):\n",
    "        # Remove \"de_novo_\" prefix\n",
    "        name = file_name[8:]  # Remove \"de_novo_\"\n",
    "        name = name.split(\".csv\")[0]  # Remove .csv and any suffixes\n",
    "        return name.replace(\"_preds\", \"\").replace(\"_results\", \"\")\n",
    "    elif file_name.startswith(\"raw_\"):\n",
    "        # Remove \"raw_\" prefix\n",
    "        name = file_name[4:]  # Remove \"raw_\"\n",
    "        name = name.split(\".csv\")[0]  # Remove .csv and any suffixes\n",
    "        return name.replace(\"_results\", \"\")\n",
    "\n",
    "    return file_name\n",
    "\n",
    "\n",
    "def convert_object_columns(metadata: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Convert object columns that might contain string representations of Python objects.\"\"\"\n",
    "\n",
    "    def try_convert(value):\n",
    "        try:\n",
    "            return ast.literal_eval(value)\n",
    "        except (ValueError, SyntaxError):\n",
    "            return value  # Return original if conversion fails\n",
    "\n",
    "    # Apply conversion to object (string) columns\n",
    "    for col in metadata.select_dtypes(include=[\"object\"]).columns:\n",
    "        metadata[col] = metadata[col].apply(try_convert)\n",
    "\n",
    "    return metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load labelled test set\n",
    "metadata_path = f\"../{SPECIES}_results/test_dataset.csv\"\n",
    "output_dir = os.path.dirname(metadata_path) + \"/plots\"\n",
    "dataset_name = os.path.basename(metadata_path)\n",
    "metadata = pd.read_csv(metadata_path)\n",
    "\n",
    "winnow_metrics_df = pd.read_csv(\n",
    "    f\"../{SPECIES}_results/test_dataset_calibrated_confidence_winnow_fdr.csv\"\n",
    ")\n",
    "decoy_metrics_df = pd.read_csv(\n",
    "    f\"../{SPECIES}_results/test_dataset_calibrated_confidence_dbg_fdr.csv\"\n",
    ")\n",
    "\n",
    "# -- Precision-Recall curve\n",
    "pr_fig = create_pr_curve_plot(\n",
    "    metadata,\n",
    "    \"Precision-recall curve for labelled data using database search\",\n",
    "    \"correct\",\n",
    ")\n",
    "pr_fig.savefig(\n",
    "    os.path.join(\n",
    "        output_dir, f\"{dataset_name}_labelled_precision_recall_with_db_search.png\"\n",
    "    ),\n",
    "    dpi=300,\n",
    "    bbox_inches=\"tight\",\n",
    ")\n",
    "pr_fig.savefig(\n",
    "    os.path.join(\n",
    "        output_dir, f\"{dataset_name}_labelled_precision_recall_with_db_search.pdf\"\n",
    "    ),\n",
    "    dpi=300,\n",
    "    bbox_inches=\"tight\",\n",
    ")\n",
    "plt.show()\n",
    "plt.close(pr_fig)\n",
    "\n",
    "# -- Calibration curves\n",
    "cal_fig = plot_combined_calibration_curves(\n",
    "    metadata, \"Calibration curves for labelled data using database search\", \"correct\"\n",
    ")\n",
    "cal_fig.savefig(\n",
    "    os.path.join(\n",
    "        output_dir, f\"{dataset_name}_labelled_calibration_curves_with_db_search.png\"\n",
    "    ),\n",
    "    dpi=300,\n",
    "    bbox_inches=\"tight\",\n",
    ")\n",
    "cal_fig.savefig(\n",
    "    os.path.join(\n",
    "        output_dir, f\"{dataset_name}_labelled_calibration_curves_with_db_search.pdf\"\n",
    "    ),\n",
    "    dpi=300,\n",
    "    bbox_inches=\"tight\",\n",
    ")\n",
    "plt.show()\n",
    "plt.close(cal_fig)\n",
    "\n",
    "# -- FDR accuracy\n",
    "fdr_fig = create_fdr_accuracy_plot(\n",
    "    metadata,\n",
    "    winnow_metrics_df,\n",
    "    decoy_metrics_df,\n",
    "    \"FDR accuracy for labelled data using database search\",\n",
    ")\n",
    "fdr_fig.savefig(\n",
    "    os.path.join(\n",
    "        output_dir, f\"{dataset_name}_labelled_fdr_accuracy_with_db_search.png\"\n",
    "    ),\n",
    "    dpi=300,\n",
    "    bbox_inches=\"tight\",\n",
    ")\n",
    "fdr_fig.savefig(\n",
    "    os.path.join(\n",
    "        output_dir, f\"{dataset_name}_labelled_fdr_accuracy_with_db_search.pdf\"\n",
    "    ),\n",
    "    dpi=300,\n",
    "    bbox_inches=\"tight\",\n",
    ")\n",
    "plt.show()\n",
    "plt.close(fdr_fig)\n",
    "\n",
    "confidence_fig = plot_combined_confidence_distributions(\n",
    "    metadata,\n",
    "    \"Confidence distribution for labelled data using database search\",\n",
    "    \"correct\",\n",
    ")\n",
    "fdr_fig.savefig(\n",
    "    os.path.join(\n",
    "        output_dir,\n",
    "        f\"{dataset_name}_labelled_confidence_distributions_with_db_search.png\",\n",
    "    ),\n",
    "    dpi=300,\n",
    "    bbox_inches=\"tight\",\n",
    ")\n",
    "fdr_fig.savefig(\n",
    "    os.path.join(\n",
    "        output_dir,\n",
    "        f\"{dataset_name}_labelled_confidence_distributions_with_db_search.pdf\",\n",
    "    ),\n",
    "    dpi=300,\n",
    "    bbox_inches=\"tight\",\n",
    ")\n",
    "plt.show()\n",
    "plt.close(confidence_fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load labelled test set\n",
    "metadata_path = f\"../{SPECIES}_results/test_dataset.csv\"\n",
    "output_dir = os.path.dirname(metadata_path) + \"/plots\"\n",
    "dataset_name = os.path.basename(metadata_path)\n",
    "metadata = pd.read_csv(metadata_path)\n",
    "\n",
    "# -- Load Winnow metrics\n",
    "winnow_metrics_df = pd.read_csv(\n",
    "    f\"../{SPECIES}_results/test_dataset_calibrated_confidence_winnow_fdr.csv\"\n",
    ")\n",
    "\n",
    "# -- Compute decoy metrics\n",
    "database_grounded_fdr_control = DatabaseGroundedFDRControl(\n",
    "    confidence_feature=\"calibrated_confidence\"\n",
    ")\n",
    "database_grounded_fdr_control.fit(\n",
    "    dataset=metadata, residue_masses=RESIDUE_MASSES, correct_column=\"proteome_hit\"\n",
    ")\n",
    "decoy_metrics_df = database_grounded_fdr_control.add_psm_fdr(\n",
    "    metadata, \"calibrated_confidence\"\n",
    ")\n",
    "decoy_metrics_df = decoy_metrics_df[[\"spectrum_id\", \"psm_fdr\"]]\n",
    "\n",
    "# -- Precision-Recall curve\n",
    "pr_fig = create_pr_curve_plot(\n",
    "    metadata,\n",
    "    \"Precision-recall curve for labelled data using proteome mapping\",\n",
    "    \"proteome_hit\",\n",
    ")\n",
    "pr_fig.savefig(\n",
    "    os.path.join(output_dir, f\"{dataset_name}_labelled_precision_recall.png\"),\n",
    "    dpi=300,\n",
    "    bbox_inches=\"tight\",\n",
    ")\n",
    "pr_fig.savefig(\n",
    "    os.path.join(output_dir, f\"{dataset_name}_labelled_precision_recall.pdf\"),\n",
    "    dpi=300,\n",
    "    bbox_inches=\"tight\",\n",
    ")\n",
    "plt.show()\n",
    "plt.close(pr_fig)\n",
    "\n",
    "# -- Calibration curves\n",
    "cal_fig = plot_combined_calibration_curves(\n",
    "    metadata,\n",
    "    \"Calibration curves for labelled data using proteome mapping\",\n",
    "    \"proteome_hit\",\n",
    ")\n",
    "cal_fig.savefig(\n",
    "    os.path.join(output_dir, f\"{dataset_name}_labelled_calibration_curves.png\"),\n",
    "    dpi=300,\n",
    "    bbox_inches=\"tight\",\n",
    ")\n",
    "cal_fig.savefig(\n",
    "    os.path.join(output_dir, f\"{dataset_name}_labelled_calibration_curves.pdf\"),\n",
    "    dpi=300,\n",
    "    bbox_inches=\"tight\",\n",
    ")\n",
    "plt.show()\n",
    "plt.close(cal_fig)\n",
    "\n",
    "# -- FDR accuracy\n",
    "fdr_fig = create_fdr_accuracy_plot(\n",
    "    metadata,\n",
    "    winnow_metrics_df,\n",
    "    decoy_metrics_df,\n",
    "    \"FDR accuracy for labelled data using proteome mapping\",\n",
    ")\n",
    "fdr_fig.savefig(\n",
    "    os.path.join(output_dir, f\"{dataset_name}_labelled_fdr_accuracy.png\"),\n",
    "    dpi=300,\n",
    "    bbox_inches=\"tight\",\n",
    ")\n",
    "fdr_fig.savefig(\n",
    "    os.path.join(output_dir, f\"{dataset_name}_labelled_fdr_accuracy.pdf\"),\n",
    "    dpi=300,\n",
    "    bbox_inches=\"tight\",\n",
    ")\n",
    "plt.show()\n",
    "plt.close(fdr_fig)\n",
    "\n",
    "confidence_fig = plot_combined_confidence_distributions(\n",
    "    metadata,\n",
    "    \"Confidence distribution for labelled data using proteome mapping\",\n",
    "    \"proteome_hit\",\n",
    ")\n",
    "confidence_fig.savefig(\n",
    "    os.path.join(output_dir, f\"{dataset_name}_labelled_confidence_distributions.png\"),\n",
    "    dpi=300,\n",
    "    bbox_inches=\"tight\",\n",
    ")\n",
    "confidence_fig.savefig(\n",
    "    os.path.join(output_dir, f\"{dataset_name}_labelled_confidence_distributions.pdf\"),\n",
    "    dpi=300,\n",
    "    bbox_inches=\"tight\",\n",
    ")\n",
    "plt.show()\n",
    "plt.close(confidence_fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load raw dataset\n",
    "metadata_path = f\"../{SPECIES}_results/raw_less_train.csv\"\n",
    "output_dir = os.path.dirname(metadata_path) + \"/plots\"\n",
    "dataset_name = os.path.basename(metadata_path)\n",
    "metadata = pd.read_csv(metadata_path)\n",
    "\n",
    "# -- Load Winnow metrics\n",
    "winnow_metrics_df = pd.read_csv(\n",
    "    f\"../{SPECIES}_results/raw_less_train_dataset_calibrated_confidence_winnow_fdr.csv\"\n",
    ")\n",
    "\n",
    "# -- Compute decoy metrics\n",
    "database_grounded_fdr_control = DatabaseGroundedFDRControl(\n",
    "    confidence_feature=\"calibrated_confidence\"\n",
    ")\n",
    "database_grounded_fdr_control.fit(\n",
    "    dataset=metadata, residue_masses=RESIDUE_MASSES, correct_column=\"proteome_hit\"\n",
    ")\n",
    "decoy_metrics_df = database_grounded_fdr_control.add_psm_fdr(\n",
    "    metadata, \"calibrated_confidence\"\n",
    ")\n",
    "decoy_metrics_df = decoy_metrics_df[[\"spectrum_id\", \"psm_fdr\"]]\n",
    "\n",
    "# -- Precision-Recall curve\n",
    "pr_fig = create_pr_curve_plot(\n",
    "    metadata,\n",
    "    \"Precision-recall curve for raw data using proteome mapping\",\n",
    "    \"proteome_hit\",\n",
    ")\n",
    "pr_fig.savefig(\n",
    "    os.path.join(output_dir, f\"{dataset_name}_raw_precision_recall.png\"),\n",
    "    dpi=300,\n",
    "    bbox_inches=\"tight\",\n",
    ")\n",
    "pr_fig.savefig(\n",
    "    os.path.join(output_dir, f\"{dataset_name}_raw_precision_recall.pdf\"),\n",
    "    dpi=300,\n",
    "    bbox_inches=\"tight\",\n",
    ")\n",
    "plt.show()\n",
    "plt.close(pr_fig)\n",
    "\n",
    "# -- Calibration curves\n",
    "cal_fig = plot_combined_calibration_curves(\n",
    "    metadata, \"Calibration curves for raw data using proteome mapping\", \"proteome_hit\"\n",
    ")\n",
    "cal_fig.savefig(\n",
    "    os.path.join(output_dir, f\"{dataset_name}_raw_calibration_curves.png\"),\n",
    "    dpi=300,\n",
    "    bbox_inches=\"tight\",\n",
    ")\n",
    "cal_fig.savefig(\n",
    "    os.path.join(output_dir, f\"{dataset_name}_raw_calibration_curves.pdf\"),\n",
    "    dpi=300,\n",
    "    bbox_inches=\"tight\",\n",
    ")\n",
    "plt.show()\n",
    "plt.close(cal_fig)\n",
    "\n",
    "# -- FDR accuracy\n",
    "fdr_fig = create_fdr_accuracy_plot(\n",
    "    metadata,\n",
    "    winnow_metrics_df,\n",
    "    decoy_metrics_df,\n",
    "    \"FDR accuracy for raw data using proteome mapping\",\n",
    ")\n",
    "fdr_fig.savefig(\n",
    "    os.path.join(output_dir, f\"{dataset_name}_raw_fdr_accuracy.png\"),\n",
    "    dpi=300,\n",
    "    bbox_inches=\"tight\",\n",
    ")\n",
    "fdr_fig.savefig(\n",
    "    os.path.join(output_dir, f\"{dataset_name}_raw_fdr_accuracy.pdf\"),\n",
    "    dpi=300,\n",
    "    bbox_inches=\"tight\",\n",
    ")\n",
    "plt.show()\n",
    "plt.close(fdr_fig)\n",
    "\n",
    "confidence_fig = plot_combined_confidence_distributions(\n",
    "    metadata,\n",
    "    \"Confidence distribution for raw data using proteome mapping\",\n",
    "    \"proteome_hit\",\n",
    ")\n",
    "confidence_fig.savefig(\n",
    "    os.path.join(output_dir, f\"{dataset_name}_raw_confidence_distributions.png\"),\n",
    "    dpi=300,\n",
    "    bbox_inches=\"tight\",\n",
    ")\n",
    "confidence_fig.savefig(\n",
    "    os.path.join(output_dir, f\"{dataset_name}_raw_confidence_distributions.pdf\"),\n",
    "    dpi=300,\n",
    "    bbox_inches=\"tight\",\n",
    ")\n",
    "plt.show()\n",
    "plt.close(confidence_fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load de novo dataset\n",
    "metadata_path = f\"../{SPECIES}_results/raw_filtered_dataset.csv\"\n",
    "output_dir = os.path.dirname(metadata_path) + \"/plots\"\n",
    "dataset_name = os.path.basename(metadata_path)\n",
    "metadata = pd.read_csv(metadata_path)\n",
    "\n",
    "# -- Load Winnow metrics\n",
    "winnow_metrics_df = pd.read_csv(\n",
    "    f\"../{SPECIES}_results/de_novo_dataset_calibrated_confidence_winnow_fdr.csv\"\n",
    ")\n",
    "\n",
    "# -- Compute decoy metrics\n",
    "database_grounded_fdr_control = DatabaseGroundedFDRControl(\n",
    "    confidence_feature=\"calibrated_confidence\"\n",
    ")\n",
    "database_grounded_fdr_control.fit(\n",
    "    dataset=metadata, residue_masses=RESIDUE_MASSES, correct_column=\"proteome_hit\"\n",
    ")\n",
    "decoy_metrics_df = database_grounded_fdr_control.add_psm_fdr(\n",
    "    metadata, \"calibrated_confidence\"\n",
    ")\n",
    "decoy_metrics_df = decoy_metrics_df[[\"spectrum_id\", \"psm_fdr\"]]\n",
    "\n",
    "# -- Precision-Recall curve\n",
    "pr_fig = create_pr_curve_plot(\n",
    "    metadata,\n",
    "    \"Precision-recall curve for \"\n",
    "    + r\"$\\mathit{de\\ novo}$\"\n",
    "    + \" data using proteome mapping\",\n",
    "    \"proteome_hit\",\n",
    ")\n",
    "pr_fig.savefig(\n",
    "    os.path.join(output_dir, f\"{dataset_name}_de_novo_precision_recall.png\"),\n",
    "    dpi=300,\n",
    "    bbox_inches=\"tight\",\n",
    ")\n",
    "pr_fig.savefig(\n",
    "    os.path.join(output_dir, f\"{dataset_name}_de_novo_precision_recall.pdf\"),\n",
    "    dpi=300,\n",
    "    bbox_inches=\"tight\",\n",
    ")\n",
    "plt.show()\n",
    "plt.close(pr_fig)\n",
    "\n",
    "# -- Calibration curves\n",
    "cal_fig = plot_combined_calibration_curves(\n",
    "    metadata,\n",
    "    \"Calibration curves for \" + r\"$\\mathit{de\\ novo}$\" + \" data using proteome mapping\",\n",
    "    \"proteome_hit\",\n",
    ")\n",
    "cal_fig.savefig(\n",
    "    os.path.join(output_dir, f\"{dataset_name}_de_novo_calibration_curves.png\"),\n",
    "    dpi=300,\n",
    "    bbox_inches=\"tight\",\n",
    ")\n",
    "cal_fig.savefig(\n",
    "    os.path.join(output_dir, f\"{dataset_name}_de_novo_calibration_curves.pdf\"),\n",
    "    dpi=300,\n",
    "    bbox_inches=\"tight\",\n",
    ")\n",
    "plt.show()\n",
    "plt.close(cal_fig)\n",
    "\n",
    "# -- FDR accuracy\n",
    "fdr_fig = create_fdr_accuracy_plot(\n",
    "    metadata,\n",
    "    winnow_metrics_df,\n",
    "    decoy_metrics_df,\n",
    "    \"FDR accuracy for \" + r\"$\\mathit{de\\ novo}$\" + \" data using proteome mapping\",\n",
    ")\n",
    "fdr_fig.savefig(\n",
    "    os.path.join(output_dir, f\"{dataset_name}_de_novo_fdr_accuracy.png\"),\n",
    "    dpi=300,\n",
    "    bbox_inches=\"tight\",\n",
    ")\n",
    "fdr_fig.savefig(\n",
    "    os.path.join(output_dir, f\"{dataset_name}_de_novo_fdr_accuracy.pdf\"),\n",
    "    dpi=300,\n",
    "    bbox_inches=\"tight\",\n",
    ")\n",
    "plt.show()\n",
    "plt.close(fdr_fig)\n",
    "\n",
    "confidence_fig = plot_combined_confidence_distributions(\n",
    "    metadata,\n",
    "    \"Confidence distribution for \"\n",
    "    + r\"$\\mathit{de\\ novo}$\"\n",
    "    + \" data using proteome mapping\",\n",
    "    \"proteome_hit\",\n",
    ")\n",
    "confidence_fig.savefig(\n",
    "    os.path.join(output_dir, f\"{dataset_name}_de_novo_confidence_distributions.png\"),\n",
    "    dpi=300,\n",
    "    bbox_inches=\"tight\",\n",
    ")\n",
    "confidence_fig.savefig(\n",
    "    os.path.join(output_dir, f\"{dataset_name}_de_novo_confidence_distributions.pdf\"),\n",
    "    dpi=300,\n",
    "    bbox_inches=\"tight\",\n",
    ")\n",
    "plt.show()\n",
    "plt.close(confidence_fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for feature in feature_columns:\n",
    "#     plt.hist(raw_metadata[raw_metadata[\"proteome_hit\"] == True][feature], label=\"Unlabelled\", color=\"green\", alpha=0.5, bins=50, density=True)\n",
    "#     plt.hist(labelled_metadata[labelled_metadata[\"proteome_hit\"] == True][feature], label=\"Labelled\", color=\"red\", alpha=0.5, bins=50, density=True)\n",
    "#     plt.xlabel(feature)\n",
    "#     plt.ylabel(\"Density\")\n",
    "#     plt.legend()\n",
    "#     plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FDR Metrics for External Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "\n",
    "\n",
    "def _map_l_to_i_in_sequences(metadata: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Map L to I in sequences and predictions.\"\"\"\n",
    "    logger.info(\"Mapping L to I in sequences and predictions\")\n",
    "\n",
    "    def _replace_l_with_i(value):\n",
    "        \"\"\"Replace L with I in a value, handling both strings and lists.\"\"\"\n",
    "        if isinstance(value, str):\n",
    "            return value.replace(\"L\", \"I\")\n",
    "        elif isinstance(value, list):\n",
    "            return [\n",
    "                token.replace(\"L\", \"I\") if isinstance(token, str) else token\n",
    "                for token in value\n",
    "            ]\n",
    "        return value\n",
    "\n",
    "    for col in [\"sequence\", \"prediction\"]:\n",
    "        if col in metadata.columns:\n",
    "            metadata[col] = metadata[col].apply(_replace_l_with_i)\n",
    "\n",
    "    return metadata\n",
    "\n",
    "\n",
    "def _convert_object_columns(metadata: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Convert object columns that might contain string representations of Python objects.\"\"\"\n",
    "\n",
    "    def try_convert(value):\n",
    "        try:\n",
    "            return ast.literal_eval(value)\n",
    "        except (ValueError, SyntaxError):\n",
    "            return value  # Return original if conversion fails\n",
    "\n",
    "    # Apply conversion to object (string) columns\n",
    "    for col in metadata.select_dtypes(include=[\"object\"]).columns:\n",
    "        metadata[col] = metadata[col].apply(try_convert)\n",
    "\n",
    "    return metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## C. elegans (PXD014877)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Labelled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labelled_df = pd.read_csv(\"../new_model/results/labelled_PXD014877_results.csv\")\n",
    "\n",
    "# Convert object columns\n",
    "labelled_df = _convert_object_columns(labelled_df)\n",
    "\n",
    "# Map L to I in sequences and predictions\n",
    "labelled_df = _map_l_to_i_in_sequences(labelled_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "non_parametric_fdr_control = NonParametricFDRControl()\n",
    "non_parametric_fdr_control.fit(dataset=labelled_df[\"confidence\"])\n",
    "\n",
    "logger.info(\n",
    "    f\"Winnow FDR threshold for raw confidence: {non_parametric_fdr_control.get_confidence_cutoff(threshold=0.05)}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "non_parametric_fdr_control = NonParametricFDRControl()\n",
    "non_parametric_fdr_control.fit(dataset=labelled_df[\"calibrated_confidence\"])\n",
    "\n",
    "logger.info(\n",
    "    f\"Winnow FDR threshold for calibrated confidence: {non_parametric_fdr_control.get_confidence_cutoff(threshold=0.05)}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "database_grounded_fdr_control = DatabaseGroundedFDRControl(\n",
    "    confidence_feature=\"confidence\"\n",
    ")\n",
    "database_grounded_fdr_control.fit(dataset=labelled_df, residue_masses=RESIDUE_MASSES)\n",
    "\n",
    "logger.info(\n",
    "    f\"Database-grounded FDR threshold for raw confidence: {database_grounded_fdr_control.get_confidence_cutoff(threshold=0.05)}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "database_grounded_fdr_control = DatabaseGroundedFDRControl(\n",
    "    confidence_feature=\"calibrated_confidence\"\n",
    ")\n",
    "database_grounded_fdr_control.fit(dataset=labelled_df, residue_masses=RESIDUE_MASSES)\n",
    "\n",
    "logger.info(\n",
    "    f\"Database-grounded FDR threshold for calibrated confidence: {database_grounded_fdr_control.get_confidence_cutoff(threshold=0.05)}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "de_novo_df = pd.read_csv(\"../new_model/results/raw_PXD014877_results.csv\")\n",
    "\n",
    "# Convert object columns\n",
    "de_novo_df = _convert_object_columns(de_novo_df)\n",
    "\n",
    "# Map L to I in sequences and predictions\n",
    "de_novo_df = _map_l_to_i_in_sequences(de_novo_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "non_parametric_fdr_control = NonParametricFDRControl()\n",
    "non_parametric_fdr_control.fit(dataset=de_novo_df[\"confidence\"])\n",
    "\n",
    "logger.info(\n",
    "    f\"Winnow FDR threshold for raw confidence: {non_parametric_fdr_control.get_confidence_cutoff(threshold=0.05)}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "non_parametric_fdr_control = NonParametricFDRControl()\n",
    "non_parametric_fdr_control.fit(dataset=de_novo_df[\"calibrated_confidence\"])\n",
    "\n",
    "logger.info(\n",
    "    f\"Winnow FDR threshold for calibrated confidence: {non_parametric_fdr_control.get_confidence_cutoff(threshold=0.05)}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Immuno-2 (PXD023064)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Labelled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labelled_df = pd.read_csv(\"../new_model/results/labelled_PXD023064_results.csv\")\n",
    "\n",
    "# Convert object columns\n",
    "labelled_df = _convert_object_columns(labelled_df)\n",
    "\n",
    "# Map L to I in sequences and predictions\n",
    "labelled_df = _map_l_to_i_in_sequences(labelled_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "non_parametric_fdr_control = NonParametricFDRControl()\n",
    "non_parametric_fdr_control.fit(dataset=labelled_df[\"confidence\"])\n",
    "\n",
    "logger.info(\n",
    "    f\"Winnow FDR threshold for raw confidence: {non_parametric_fdr_control.get_confidence_cutoff(threshold=0.05)}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "non_parametric_fdr_control = NonParametricFDRControl()\n",
    "non_parametric_fdr_control.fit(dataset=labelled_df[\"calibrated_confidence\"])\n",
    "\n",
    "logger.info(\n",
    "    f\"Winnow FDR threshold for calibrated confidence: {non_parametric_fdr_control.get_confidence_cutoff(threshold=0.05)}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "database_grounded_fdr_control = DatabaseGroundedFDRControl(\n",
    "    confidence_feature=\"confidence\"\n",
    ")\n",
    "database_grounded_fdr_control.fit(dataset=labelled_df, residue_masses=RESIDUE_MASSES)\n",
    "\n",
    "logger.info(\n",
    "    f\"Database-grounded FDR threshold for raw confidence: {database_grounded_fdr_control.get_confidence_cutoff(threshold=0.05)}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "database_grounded_fdr_control = DatabaseGroundedFDRControl(\n",
    "    confidence_feature=\"calibrated_confidence\"\n",
    ")\n",
    "database_grounded_fdr_control.fit(dataset=labelled_df, residue_masses=RESIDUE_MASSES)\n",
    "\n",
    "logger.info(\n",
    "    f\"Database-grounded FDR threshold for calibrated confidence: {database_grounded_fdr_control.get_confidence_cutoff(threshold=0.05)}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "de_novo_df = pd.read_csv(\"../new_model/results/raw_PXD023064_results.csv\")\n",
    "\n",
    "# Convert object columns\n",
    "de_novo_df = _convert_object_columns(de_novo_df)\n",
    "\n",
    "# Map L to I in sequences and predictions\n",
    "de_novo_df = _map_l_to_i_in_sequences(de_novo_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "non_parametric_fdr_control = NonParametricFDRControl()\n",
    "non_parametric_fdr_control.fit(dataset=de_novo_df[\"confidence\"])\n",
    "\n",
    "logger.info(\n",
    "    f\"Winnow FDR threshold for raw confidence: {non_parametric_fdr_control.get_confidence_cutoff(threshold=0.05)}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "non_parametric_fdr_control = NonParametricFDRControl()\n",
    "non_parametric_fdr_control.fit(dataset=de_novo_df[\"calibrated_confidence\"])\n",
    "\n",
    "logger.info(\n",
    "    f\"Winnow FDR threshold for calibrated confidence: {non_parametric_fdr_control.get_confidence_cutoff(threshold=0.05)}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## General model test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labelled_df = pd.read_csv(\"../new_model/results/labelled_general_results.csv\")\n",
    "\n",
    "# Convert object columns\n",
    "labelled_df = _convert_object_columns(labelled_df)\n",
    "\n",
    "# Map L to I in sequences and predictions\n",
    "labelled_df = _map_l_to_i_in_sequences(labelled_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "non_parametric_fdr_control = NonParametricFDRControl()\n",
    "non_parametric_fdr_control.fit(dataset=labelled_df[\"confidence\"])\n",
    "\n",
    "logger.info(\n",
    "    f\"Winnow FDR threshold for raw confidence: {non_parametric_fdr_control.get_confidence_cutoff(threshold=0.05)}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "non_parametric_fdr_control = NonParametricFDRControl()\n",
    "non_parametric_fdr_control.fit(dataset=labelled_df[\"calibrated_confidence\"])\n",
    "\n",
    "logger.info(\n",
    "    f\"Winnow FDR threshold for calibrated confidence: {non_parametric_fdr_control.get_confidence_cutoff(threshold=0.05)}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "database_grounded_fdr_control = DatabaseGroundedFDRControl(\n",
    "    confidence_feature=\"confidence\"\n",
    ")\n",
    "database_grounded_fdr_control.fit(dataset=labelled_df, residue_masses=RESIDUE_MASSES)\n",
    "\n",
    "logger.info(\n",
    "    f\"Database-grounded FDR threshold for raw confidence: {database_grounded_fdr_control.get_confidence_cutoff(threshold=0.05)}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "database_grounded_fdr_control = DatabaseGroundedFDRControl(\n",
    "    confidence_feature=\"calibrated_confidence\"\n",
    ")\n",
    "database_grounded_fdr_control.fit(dataset=labelled_df, residue_masses=RESIDUE_MASSES)\n",
    "\n",
    "logger.info(\n",
    "    f\"Database-grounded FDR threshold for calibrated confidence: {database_grounded_fdr_control.get_confidence_cutoff(threshold=0.05)}\"\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
