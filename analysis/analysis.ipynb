{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- Import\n",
    "from winnow.datasets.calibration_dataset import RESIDUE_MASSES\n",
    "from winnow.datasets.data_loaders import InstaNovoDatasetLoader\n",
    "from winnow.calibration.calibrator import ProbabilityCalibrator\n",
    "from winnow.scripts.main import (\n",
    "    filter_dataset,\n",
    "    initialise_calibrator,\n",
    ")\n",
    "from winnow.fdr.database_grounded import DatabaseGroundedFDRControl\n",
    "from winnow.fdr.nonparametric import NonParametricFDRControl\n",
    "from huggingface_hub import snapshot_download, list_repo_files\n",
    "import numpy as np\n",
    "import logging\n",
    "import os\n",
    "import ast\n",
    "import glob\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.calibration import calibration_curve\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- Set up logging\n",
    "logger = logging.getLogger(__name__)\n",
    "logger.setLevel(logging.INFO)\n",
    "logger.addHandler(logging.StreamHandler())\n",
    "\n",
    "# -- Set the random seed\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "repo_id = \"InstaDeepAI/winnow-ms-datasets\"\n",
    "save_dir = \"../winnow-ms-datasets\"\n",
    "\n",
    "files = list_repo_files(repo_id=repo_id, repo_type=\"dataset\")\n",
    "print([f for f in files])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- Download the helaqc dataset\n",
    "snapshot_download(\n",
    "    repo_id=repo_id,\n",
    "    repo_type=\"dataset\",\n",
    "    allow_patterns=[\"helaqc*.parquet\", \"helaqc*.csv\"],\n",
    "    local_dir=save_dir,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- Load data\n",
    "logger.info(\"Loading dataset.\")\n",
    "dataset = InstaNovoDatasetLoader().load(\n",
    "    \"../winnow-ms-datasets/helaqc_labelled.parquet\",\n",
    "    \"../winnow-ms-datasets/helaqc_labelled_beams.csv\",\n",
    ")\n",
    "\n",
    "logger.info(\"Filtering dataset.\")\n",
    "filtered_dataset = filter_dataset(dataset)\n",
    "\n",
    "train_dataset = filtered_dataset.filter_entries(\n",
    "    metadata_predicate=lambda row: row[\"split\"] == \"test\"\n",
    ")\n",
    "test_dataset = filtered_dataset.filter_entries(\n",
    "    metadata_predicate=lambda row: row[\"split\"] == \"train\"\n",
    ")\n",
    "\n",
    "print(\"Number of spectra in train set:\", len(train_dataset))\n",
    "print(\"Number of spectra in test set:\", len(test_dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger.info(\"Training calibrator.\")\n",
    "calibrator = initialise_calibrator()\n",
    "calibrator.fit(train_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ProbabilityCalibrator.save(calibrator, Path(\"../models/helaqc_model\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate model on labelled test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger.info(\"Calibrating scores.\")\n",
    "calibrator.predict(test_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger.info(\"Saving evaluation results.\")\n",
    "test_dataset.metadata.to_csv(\"../results/helaqc_results/test_dataset.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute FDR metrics on calibrated confidence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "non_parametric_fdr_control = NonParametricFDRControl()\n",
    "non_parametric_fdr_control.fit(dataset=test_dataset.metadata[\"calibrated_confidence\"])\n",
    "\n",
    "logger.info(\n",
    "    f\"Winnow confidence cutoff for FDR 0.05 using calibrated confidence: {non_parametric_fdr_control.get_confidence_cutoff(threshold=0.05)}\"\n",
    ")\n",
    "\n",
    "test_dataset_metadata = non_parametric_fdr_control.add_psm_fdr(\n",
    "    test_dataset.metadata, \"calibrated_confidence\"\n",
    ")\n",
    "test_dataset_metadata = non_parametric_fdr_control.add_psm_pep(\n",
    "    test_dataset_metadata, \"calibrated_confidence\"\n",
    ")\n",
    "test_dataset_metadata = non_parametric_fdr_control.add_psm_qvalue(\n",
    "    test_dataset_metadata, \"calibrated_confidence\"\n",
    ")\n",
    "\n",
    "# Save metrics\n",
    "test_dataset_metadata[[\"spectrum_id\", \"psm_fdr\", \"psm_pep\", \"psm_qvalue\"]].to_csv(\n",
    "    \"../results/helaqc_results/test_dataset_calibrated_confidence_winnow_fdr.csv\",\n",
    "    index=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute database-grounded FDR metrics on raw confidence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "database_grounded_fdr_control = DatabaseGroundedFDRControl(\n",
    "    confidence_feature=\"confidence\"\n",
    ")\n",
    "database_grounded_fdr_control.fit(\n",
    "    dataset=test_dataset.metadata, residue_masses=RESIDUE_MASSES\n",
    ")\n",
    "\n",
    "logger.info(\n",
    "    f\"Database-grounded confidence cutoff for FDR 0.05 using raw confidence: {database_grounded_fdr_control.get_confidence_cutoff(threshold=0.05)}\"\n",
    ")\n",
    "\n",
    "test_dataset_metadata = database_grounded_fdr_control.add_psm_fdr(\n",
    "    test_dataset.metadata, \"confidence\"\n",
    ")\n",
    "test_dataset_metadata = database_grounded_fdr_control.add_psm_qvalue(\n",
    "    test_dataset_metadata, \"confidence\"\n",
    ")\n",
    "\n",
    "# Save metrics\n",
    "test_dataset_metadata[[\"spectrum_id\", \"psm_fdr\", \"psm_qvalue\"]].to_csv(\n",
    "    \"../results/helaqc_results/test_dataset_raw_confidence_dbg_fdr.csv\", index=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute database-grounded FDR metrics on calibrated confidence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "database_grounded_fdr_control = DatabaseGroundedFDRControl(\n",
    "    confidence_feature=\"calibrated_confidence\"\n",
    ")\n",
    "database_grounded_fdr_control.fit(\n",
    "    dataset=test_dataset.metadata, residue_masses=RESIDUE_MASSES\n",
    ")\n",
    "\n",
    "logger.info(\n",
    "    f\"Database-grounded confidence cutoff for FDR 0.05 using calibrated confidence: {database_grounded_fdr_control.get_confidence_cutoff(threshold=0.05)}\"\n",
    ")\n",
    "\n",
    "test_dataset_metadata = database_grounded_fdr_control.add_psm_fdr(\n",
    "    test_dataset.metadata, \"calibrated_confidence\"\n",
    ")\n",
    "test_dataset_metadata = database_grounded_fdr_control.add_psm_qvalue(\n",
    "    test_dataset_metadata, \"calibrated_confidence\"\n",
    ")\n",
    "\n",
    "# Save metrics\n",
    "test_dataset_metadata[[\"spectrum_id\", \"psm_fdr\", \"psm_qvalue\"]].to_csv(\n",
    "    \"../results/helaqc_results/test_dataset_calibrated_confidence_dbg_fdr.csv\",\n",
    "    index=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate model on full search space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- Load the raw, unlabelled data\n",
    "logger.info(\"Loading raw dataset.\")\n",
    "raw_dataset = InstaNovoDatasetLoader().load(\n",
    "    \"../winnow-ms-datasets/helaqc_raw_less_train.parquet\",\n",
    "    \"../winnow-ms-datasets/helaqc_raw_less_train_beams.csv\",\n",
    ")\n",
    "\n",
    "logger.info(\"Filtering dataset.\")\n",
    "raw_filtered_dataset = filter_dataset(raw_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger.info(\"Calibrating scores.\")\n",
    "calibrator.predict(raw_filtered_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger.info(\"Saving evaluation results.\")\n",
    "raw_filtered_dataset.metadata.to_csv(\n",
    "    \"../results/helaqc_results/raw_less_train.csv\", index=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute FDR metrics on calibrated confidence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "non_parametric_fdr_control = NonParametricFDRControl()\n",
    "non_parametric_fdr_control.fit(\n",
    "    dataset=raw_filtered_dataset.metadata[\"calibrated_confidence\"]\n",
    ")\n",
    "\n",
    "logger.info(\n",
    "    f\"Winnow FDR threshold for calibrated confidence: {non_parametric_fdr_control.get_confidence_cutoff(threshold=0.05)}\"\n",
    ")\n",
    "\n",
    "raw_filtered_dataset_metadata = non_parametric_fdr_control.add_psm_fdr(\n",
    "    raw_filtered_dataset.metadata, \"calibrated_confidence\"\n",
    ")\n",
    "raw_filtered_dataset_metadata = non_parametric_fdr_control.add_psm_pep(\n",
    "    raw_filtered_dataset_metadata, \"calibrated_confidence\"\n",
    ")\n",
    "raw_filtered_dataset_metadata = non_parametric_fdr_control.add_psm_qvalue(\n",
    "    raw_filtered_dataset_metadata, \"calibrated_confidence\"\n",
    ")\n",
    "\n",
    "# Save metrics\n",
    "raw_filtered_dataset_metadata[\n",
    "    [\"spectrum_id\", \"psm_fdr\", \"psm_pep\", \"psm_qvalue\"]\n",
    "].to_csv(\n",
    "    \"../results/helaqc_results/raw_less_train_dataset_calibrated_confidence_winnow_fdr.csv\",\n",
    "    index=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_theme(style=\"white\", palette=\"colorblind\", context=\"paper\", font_scale=1.5)\n",
    "\n",
    "COLORS = {\n",
    "    \"fairy\": \"#FFCAE9\",\n",
    "    \"magenta\": \"#8E5572\",\n",
    "    \"ash\": \"#BBC5AA\",\n",
    "    \"ebony\": \"#5A6650\",\n",
    "    \"sky\": \"#7FC8F8\",\n",
    "    \"navy\": \"#3C81AE\",\n",
    "}\n",
    "\n",
    "# Species name mapping for nicer plot labels\n",
    "SPECIES_NAME_MAPPING = {\n",
    "    \"gluc\": \"HeLa degradome\",\n",
    "    \"helaqc\": \"HeLa single shot\",\n",
    "    \"herceptin\": \"Herceptin\",\n",
    "    \"immuno\": \"Immunopeptidomics-1\",\n",
    "    \"sbrodae\": \"Scalindua brodae\",\n",
    "    \"snakevenoms\": \"Snake venomics\",\n",
    "    \"woundfluids\": \"Wound exudates\",\n",
    "    \"PXD014877\": \"C. elegans\",\n",
    "    \"PXD019483\": \"HepG2\",\n",
    "    \"PXD023064\": \"Immunopeptidomics-2\",\n",
    "    \"general\": \"General test set\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_pr_curve(\n",
    "    input_dataset: pd.DataFrame,\n",
    "    confidence_column: str,\n",
    "    label_column: str,\n",
    "    name: str,\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"Compute precision-recall curve for given confidence scores and labels.\n",
    "\n",
    "    Args:\n",
    "        input_dataset: DataFrame containing confidence scores and labels\n",
    "        confidence_column: Name of the column containing confidence scores\n",
    "        label_column: Name of the column containing boolean labels\n",
    "        name: Name to assign to the computed curve\n",
    "\n",
    "    Returns:\n",
    "        DataFrame with precision, recall, and name columns\n",
    "    \"\"\"\n",
    "    original = input_dataset[[confidence_column, label_column]]\n",
    "    original = original.sort_values(by=confidence_column, ascending=False)\n",
    "    cum_correct = np.cumsum(original[label_column])\n",
    "    precision = cum_correct / np.arange(1, len(original) + 1)\n",
    "    recall = cum_correct / len(original)\n",
    "    metrics = pd.DataFrame({\"precision\": precision, \"recall\": recall}).reset_index(\n",
    "        drop=True\n",
    "    )\n",
    "    metrics[\"name\"] = name\n",
    "    return metrics\n",
    "\n",
    "\n",
    "def plot_pr_curve_on_axes(\n",
    "    metadata: pd.DataFrame,\n",
    "    ax: plt.Axes,\n",
    "    title: str = \"Precision-Recall Curve\",\n",
    "    label_column: str = \"correct\",\n",
    ") -> None:\n",
    "    \"\"\"Plot precision-recall curves for original and calibrated confidence on a given axes.\"\"\"\n",
    "    # Compute PR curves\n",
    "    original = compute_pr_curve(\n",
    "        input_dataset=metadata,\n",
    "        confidence_column=\"confidence\",\n",
    "        label_column=label_column,\n",
    "        name=\"Raw confidence\",\n",
    "    )\n",
    "    calibrated = compute_pr_curve(\n",
    "        input_dataset=metadata,\n",
    "        confidence_column=\"calibrated_confidence\",\n",
    "        label_column=label_column,\n",
    "        name=\"Calibrated confidence\",\n",
    "    )\n",
    "    metrics = pd.concat([original, calibrated], axis=0).reset_index(drop=True)\n",
    "\n",
    "    # Create color palette\n",
    "    palette = {\n",
    "        \"Raw confidence\": COLORS[\"sky\"],\n",
    "        \"Calibrated confidence\": COLORS[\"ebony\"],\n",
    "    }\n",
    "\n",
    "    # Plot using seaborn\n",
    "    sns.lineplot(\n",
    "        data=metrics,\n",
    "        x=\"recall\",\n",
    "        y=\"precision\",\n",
    "        hue=\"name\",\n",
    "        palette=palette,\n",
    "        ax=ax,\n",
    "        markers=False,\n",
    "        linewidth=1.5,\n",
    "    )\n",
    "\n",
    "    ax.set_axisbelow(True)\n",
    "    ax.grid(True, color=\"lightgray\", zorder=0)\n",
    "    ax.set_xlabel(\"Recall\")\n",
    "    ax.set_ylabel(\"Precision\")\n",
    "    ax.set_title(title)\n",
    "    ax.set_xlim(0, 1)\n",
    "    ax.set_ylim(0, 1)\n",
    "    ax.legend(title=None)\n",
    "\n",
    "\n",
    "def plot_confidence_distribution_on_axes(\n",
    "    metadata: pd.DataFrame,\n",
    "    ax: plt.Axes,\n",
    "    confidence_column: str = \"confidence\",\n",
    "    title: str = \"Confidence Distribution\",\n",
    "    density: bool = False,\n",
    "    label_column: str = \"correct\",\n",
    ") -> None:\n",
    "    \"\"\"Plot confidence distribution on a given axes.\"\"\"\n",
    "    plot_df = metadata[[confidence_column, label_column]].copy(deep=True)\n",
    "    plot_df[label_column] = plot_df[label_column].apply(\n",
    "        lambda x: \"Correct\" if x else \"Incorrect\"\n",
    "    )\n",
    "\n",
    "    # Create color palette\n",
    "    palette = {\"Incorrect\": COLORS[\"sky\"], \"Correct\": COLORS[\"ebony\"]}\n",
    "\n",
    "    # Plot using seaborn histplot\n",
    "    sns.histplot(\n",
    "        data=plot_df,\n",
    "        x=confidence_column,\n",
    "        hue=label_column,\n",
    "        bins=50,\n",
    "        alpha=0.7,\n",
    "        palette=palette,\n",
    "        ax=ax,\n",
    "        stat=\"density\" if density else \"count\",\n",
    "        edgecolor=\"#333333\",\n",
    "        linewidth=0.5,\n",
    "    )\n",
    "\n",
    "    # Remove legend title while keeping the legend\n",
    "    legend = ax.get_legend()\n",
    "    if legend:\n",
    "        legend.set_title(None)\n",
    "\n",
    "    ax.set_xlabel(confidence_column.replace(\"_\", \" \").title())\n",
    "    if density:\n",
    "        ax.set_ylabel(\"Density\")\n",
    "    else:\n",
    "        ax.set_ylabel(\"Frequency\")\n",
    "    ax.set_title(title)\n",
    "\n",
    "\n",
    "def plot_calibration_curve_on_axes(\n",
    "    metadata: pd.DataFrame,\n",
    "    ax: plt.Axes,\n",
    "    confidence_column: str = \"confidence\",\n",
    "    title: str = \"Confidence Calibration\",\n",
    "    label_column: str = \"correct\",\n",
    ") -> None:\n",
    "    \"\"\"Plot probability calibration curve on a given axes.\"\"\"\n",
    "    confidence_scores = metadata[confidence_column].values\n",
    "    true_labels = metadata[label_column].values\n",
    "\n",
    "    # Calculate calibration curve\n",
    "    fraction_of_positives, mean_predicted_value = calibration_curve(\n",
    "        true_labels, confidence_scores, n_bins=10, strategy=\"uniform\"\n",
    "    )\n",
    "\n",
    "    # Determine color based on confidence column\n",
    "    if confidence_column == \"confidence\":\n",
    "        color = COLORS[\"sky\"]  # Sky for original\n",
    "        label = \"Raw confidence\"\n",
    "    else:\n",
    "        color = COLORS[\"ebony\"]  # Ebony for calibrated\n",
    "        label = \"Calibrated confidence\"\n",
    "\n",
    "    # Create calibration data for seaborn\n",
    "    cal_data = pd.DataFrame(\n",
    "        {\n",
    "            \"mean_predicted\": mean_predicted_value,\n",
    "            \"fraction_positive\": fraction_of_positives,\n",
    "            \"type\": label,\n",
    "        }\n",
    "    )\n",
    "\n",
    "    # Plot calibration curve using seaborn\n",
    "    sns.lineplot(\n",
    "        data=cal_data,\n",
    "        x=\"mean_predicted\",\n",
    "        y=\"fraction_positive\",\n",
    "        color=color,\n",
    "        marker=\"s\",\n",
    "        label=label,\n",
    "        ax=ax,\n",
    "        linewidth=1.5,\n",
    "    )\n",
    "\n",
    "    # Add perfect calibration line\n",
    "    ax.plot([0, 1], [0, 1], \"k--\", label=\"Perfectly calibrated\", alpha=0.5, zorder=2)\n",
    "    ax.set_axisbelow(True)\n",
    "    ax.grid(True, color=\"lightgray\", zorder=0)\n",
    "    ax.set_xlabel(\"Mean predicted probability\")\n",
    "    ax.set_ylabel(\"Fraction of positives\")\n",
    "    ax.set_title(title)\n",
    "    ax.legend()\n",
    "    ax.set_xlim([0, 1])\n",
    "    ax.set_ylim([0, 1])\n",
    "\n",
    "\n",
    "def plot_combined_calibration_curves(\n",
    "    metadata: pd.DataFrame,\n",
    "    title: str = \"Confidence Calibration Comparison\",\n",
    "    label_column: str = \"correct\",\n",
    ") -> plt.Figure:\n",
    "    \"\"\"Plot both original and calibrated confidence calibration curves on a single axis.\"\"\"\n",
    "    fig, ax = plt.subplots(figsize=(8, 6))\n",
    "\n",
    "    # Calculate calibration curves\n",
    "    confidence_scores = metadata[\"confidence\"].values\n",
    "    true_labels = metadata[label_column].values\n",
    "    fraction_of_positives, mean_predicted_value = calibration_curve(\n",
    "        true_labels, confidence_scores, n_bins=10, strategy=\"uniform\"\n",
    "    )\n",
    "\n",
    "    calibrated_scores = metadata[\"calibrated_confidence\"].values\n",
    "    fraction_of_positives_cal, mean_predicted_value_cal = calibration_curve(\n",
    "        true_labels, calibrated_scores, n_bins=10, strategy=\"uniform\"\n",
    "    )\n",
    "\n",
    "    # Create combined data for seaborn\n",
    "    cal_data = pd.DataFrame(\n",
    "        {\n",
    "            \"mean_predicted\": np.concatenate(\n",
    "                [mean_predicted_value, mean_predicted_value_cal]\n",
    "            ),\n",
    "            \"fraction_positive\": np.concatenate(\n",
    "                [fraction_of_positives, fraction_of_positives_cal]\n",
    "            ),\n",
    "            \"confidence_type\": [\"Raw confidence\"] * len(mean_predicted_value)\n",
    "            + [\"Calibrated confidence\"] * len(mean_predicted_value_cal),\n",
    "        }\n",
    "    )\n",
    "\n",
    "    # Create color palette\n",
    "    palette = {\n",
    "        \"Raw confidence\": COLORS[\"sky\"],\n",
    "        \"Calibrated confidence\": COLORS[\"ebony\"],\n",
    "    }\n",
    "\n",
    "    # Plot using seaborn\n",
    "    sns.lineplot(\n",
    "        data=cal_data,\n",
    "        x=\"mean_predicted\",\n",
    "        y=\"fraction_positive\",\n",
    "        hue=\"confidence_type\",\n",
    "        style=\"confidence_type\",\n",
    "        markers={\"Raw confidence\": \"s\", \"Calibrated confidence\": \"o\"},\n",
    "        dashes=False,\n",
    "        markersize=6,\n",
    "        palette=palette,\n",
    "        ax=ax,\n",
    "        linewidth=1.5,\n",
    "    )\n",
    "\n",
    "    # Perfect calibration line\n",
    "    ax.plot([0, 1], [0, 1], \"k--\", label=\"Perfectly calibrated\", alpha=0.5, zorder=2)\n",
    "\n",
    "    ax.set_axisbelow(True)\n",
    "    ax.grid(True, color=\"lightgray\", zorder=0)\n",
    "\n",
    "    ax.set_xlabel(\"Mean predicted probability\")\n",
    "    ax.set_ylabel(\"Fraction of positives\")\n",
    "    ax.set_title(title)\n",
    "    ax.legend()\n",
    "    ax.set_xlim([0, 1])\n",
    "    ax.set_ylim([0, 1])\n",
    "\n",
    "    return fig\n",
    "\n",
    "\n",
    "def plot_combined_confidence_distributions(\n",
    "    metadata: pd.DataFrame,\n",
    "    title: str = \"Confidence Distributions\",\n",
    "    label_column: str = \"correct\",\n",
    ") -> plt.Figure:\n",
    "    \"\"\"Plot both original and calibrated confidence distributions on a single panel.\"\"\"\n",
    "    # Create figure using seaborn style\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "    # Original confidence distribution\n",
    "    plot_confidence_distribution_on_axes(\n",
    "        metadata,\n",
    "        ax1,\n",
    "        \"confidence\",\n",
    "        \"Raw confidence distribution\",\n",
    "        label_column=label_column,\n",
    "    )\n",
    "\n",
    "    # Calibrated confidence distribution\n",
    "    plot_confidence_distribution_on_axes(\n",
    "        metadata,\n",
    "        ax2,\n",
    "        \"calibrated_confidence\",\n",
    "        \"Calibrated confidence distribution\",\n",
    "        label_column=label_column,\n",
    "    )\n",
    "\n",
    "    fig.suptitle(title)\n",
    "    return fig\n",
    "\n",
    "\n",
    "def get_plot_dataframe(\n",
    "    features_df: pd.DataFrame,\n",
    "    winnow_metrics_df: pd.DataFrame,\n",
    "    decoy_metrics_df: pd.DataFrame,\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"Create a dataframe for FDR plotting with true and estimated FDR values.\n",
    "\n",
    "    Args:\n",
    "        features_df: DataFrame containing features\n",
    "        winnow_metrics_df: DataFrame containing Winnow metrics\n",
    "        decoy_metrics_df: DataFrame containing decoy metrics\n",
    "\n",
    "    Returns:\n",
    "        DataFrame with confidence, FDR, and source columns\n",
    "    \"\"\"\n",
    "    metrics_df = pd.merge(\n",
    "        decoy_metrics_df,\n",
    "        winnow_metrics_df,\n",
    "        on=\"spectrum_id\",\n",
    "        how=\"inner\",\n",
    "        suffixes=(\"_dbg\", \"_winnow\"),\n",
    "    )\n",
    "    df = pd.merge(\n",
    "        features_df[[\"spectrum_id\", \"calibrated_confidence\"]],\n",
    "        metrics_df,\n",
    "        on=\"spectrum_id\",\n",
    "        how=\"inner\",\n",
    "    ).sort_values(by=\"calibrated_confidence\")\n",
    "    return df\n",
    "\n",
    "\n",
    "def plot_fdr_accuracy_on_axes(\n",
    "    metadata: pd.DataFrame,\n",
    "    winnow_metrics_df: pd.DataFrame,\n",
    "    decoy_metrics_df: pd.DataFrame,\n",
    "    ax: plt.Axes,\n",
    "    title: str = \"FDR Accuracy\",\n",
    ") -> None:\n",
    "    \"\"\"Plot FDR accuracy comparison on a given axes.\n",
    "\n",
    "    Args:\n",
    "        metadata: DataFrame containing confidence scores and labels\n",
    "        ax: Matplotlib axes to plot on\n",
    "        fdr_function: Function to calculate estimated FDR\n",
    "        confidence_column: Name of the column containing confidence scores\n",
    "        title: Title for the plot\n",
    "        label_column: Name of the column containing boolean labels\n",
    "    \"\"\"\n",
    "    # Get the multi-plot dataframe\n",
    "    multi_plot_df = get_plot_dataframe(\n",
    "        features_df=metadata,\n",
    "        winnow_metrics_df=winnow_metrics_df,\n",
    "        decoy_metrics_df=decoy_metrics_df,\n",
    "    )\n",
    "\n",
    "    # Prepare data for seaborn\n",
    "    fdr_data = pd.DataFrame(\n",
    "        {\n",
    "            \"calibrated_confidence\": np.concatenate(\n",
    "                [\n",
    "                    multi_plot_df[\"calibrated_confidence\"],\n",
    "                    multi_plot_df[\"calibrated_confidence\"],\n",
    "                ]\n",
    "            ),\n",
    "            \"fdr\": np.concatenate(\n",
    "                [multi_plot_df[\"psm_fdr_winnow\"], multi_plot_df[\"psm_fdr_dbg\"]]\n",
    "            ),\n",
    "            \"fdr_type\": [\"Winnow FDR\"] * len(multi_plot_df)\n",
    "            + [\"Decoy FDR\"] * len(multi_plot_df),\n",
    "        }\n",
    "    )\n",
    "\n",
    "    # Create color palette\n",
    "    palette = {\"Winnow FDR\": COLORS[\"sky\"], \"Decoy FDR\": COLORS[\"ebony\"]}\n",
    "\n",
    "    # Plot using seaborn\n",
    "    sns.lineplot(\n",
    "        data=fdr_data,\n",
    "        x=\"calibrated_confidence\",\n",
    "        y=\"fdr\",\n",
    "        hue=\"fdr_type\",\n",
    "        palette=palette,\n",
    "        ax=ax,\n",
    "        linewidth=1.5,\n",
    "    )\n",
    "\n",
    "    # Add horizontal line at FDR = 0.05\n",
    "    ax.axhline(y=0.05, color=\"black\", linestyle=\"--\", alpha=0.5, zorder=2)\n",
    "\n",
    "    # Customize the plot\n",
    "    ax.set_axisbelow(True)\n",
    "    ax.grid(True, color=\"lightgray\", zorder=0)\n",
    "    ax.set_xlabel(\"Calibrated Confidence\")\n",
    "    ax.set_ylabel(\"False discovery rate (FDR)\")\n",
    "    ax.set_title(title)\n",
    "    ax.legend()\n",
    "\n",
    "\n",
    "def create_fdr_accuracy_plot(\n",
    "    metadata: pd.DataFrame,\n",
    "    winnow_metrics_df: pd.DataFrame,\n",
    "    decoy_metrics_df: pd.DataFrame,\n",
    "    title: str = \"FDR Accuracy\",\n",
    ") -> plt.Figure:\n",
    "    \"\"\"Create standalone FDR accuracy plot.\"\"\"\n",
    "    # Create figure with seaborn styling\n",
    "    fig, ax = plt.subplots(figsize=(8, 6))\n",
    "    plot_fdr_accuracy_on_axes(\n",
    "        metadata,\n",
    "        winnow_metrics_df,\n",
    "        decoy_metrics_df,\n",
    "        ax,\n",
    "        title,\n",
    "    )\n",
    "    return fig\n",
    "\n",
    "\n",
    "def create_pr_curve_plot(\n",
    "    metadata: pd.DataFrame,\n",
    "    title: str = \"Precision-Recall Curve\",\n",
    "    label_column: str = \"correct\",\n",
    ") -> plt.Figure:\n",
    "    \"\"\"Create standalone precision-recall curve plot.\"\"\"\n",
    "    # Create figure with seaborn styling\n",
    "    fig, ax = plt.subplots(figsize=(8, 6))\n",
    "    plot_pr_curve_on_axes(metadata, ax, title, label_column)\n",
    "    return fig\n",
    "\n",
    "\n",
    "def find_data_files(base_dir: str = \"new_model/results\") -> dict:\n",
    "    \"\"\"Find all relevant data files in the results directory.\n",
    "\n",
    "    Args:\n",
    "        base_dir: Base directory to search for files\n",
    "\n",
    "    Returns:\n",
    "        Dictionary with file categories and their paths\n",
    "    \"\"\"\n",
    "    files: dict[str, list[str]] = {\"labelled\": [], \"de_novo\": [], \"raw\": []}\n",
    "\n",
    "    # Find all CSV files in the directory\n",
    "    csv_files = []\n",
    "    for pattern in [\"*.csv\", \"*.csv.*\"]:  # Include files with suffixes\n",
    "        csv_files.extend(glob.glob(os.path.join(base_dir, pattern)))\n",
    "\n",
    "    for file_path in csv_files:\n",
    "        file_name = os.path.basename(file_path)\n",
    "\n",
    "        if file_name.startswith(\"labelled_\"):\n",
    "            files[\"labelled\"].append(file_path)\n",
    "        elif file_name.startswith(\"de_novo_\"):\n",
    "            files[\"de_novo\"].append(file_path)\n",
    "        elif file_name.startswith(\"raw_\"):\n",
    "            files[\"raw\"].append(file_path)\n",
    "\n",
    "    return files\n",
    "\n",
    "\n",
    "def extract_dataset_name(file_path: str) -> str:\n",
    "    \"\"\"Extract dataset name from file path.\n",
    "\n",
    "    Args:\n",
    "        file_path: Path to the data file\n",
    "\n",
    "    Returns:\n",
    "        Dataset name\n",
    "    \"\"\"\n",
    "    file_name = os.path.basename(file_path)\n",
    "\n",
    "    if file_name.startswith(\"labelled_\"):\n",
    "        # Remove \"labelled_\" prefix and \".csv\" suffix (and any additional suffixes)\n",
    "        name = file_name[9:]  # Remove \"labelled_\"\n",
    "        name = name.split(\".csv\")[0]  # Remove .csv and any suffixes\n",
    "        return name.replace(\"_results\", \"\")\n",
    "    elif file_name.startswith(\"de_novo_\"):\n",
    "        # Remove \"de_novo_\" prefix\n",
    "        name = file_name[8:]  # Remove \"de_novo_\"\n",
    "        name = name.split(\".csv\")[0]  # Remove .csv and any suffixes\n",
    "        return name.replace(\"_preds\", \"\").replace(\"_results\", \"\")\n",
    "    elif file_name.startswith(\"raw_\"):\n",
    "        # Remove \"raw_\" prefix\n",
    "        name = file_name[4:]  # Remove \"raw_\"\n",
    "        name = name.split(\".csv\")[0]  # Remove .csv and any suffixes\n",
    "        return name.replace(\"_results\", \"\")\n",
    "\n",
    "    return file_name\n",
    "\n",
    "\n",
    "def convert_object_columns(metadata: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Convert object columns that might contain string representations of Python objects.\"\"\"\n",
    "\n",
    "    def try_convert(value):\n",
    "        try:\n",
    "            return ast.literal_eval(value)\n",
    "        except (ValueError, SyntaxError):\n",
    "            return value  # Return original if conversion fails\n",
    "\n",
    "    # Apply conversion to object (string) columns\n",
    "    for col in metadata.select_dtypes(include=[\"object\"]).columns:\n",
    "        metadata[col] = metadata[col].apply(try_convert)\n",
    "\n",
    "    return metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load labelled test set\n",
    "metadata_path = \"../results/helaqc_results/test_dataset.csv\"\n",
    "output_dir = os.path.dirname(metadata_path) + \"/plots\"\n",
    "dataset_name = os.path.basename(metadata_path)\n",
    "metadata = pd.read_csv(metadata_path)\n",
    "\n",
    "winnow_metrics_df = pd.read_csv(\n",
    "    \"../results/helaqc_results/test_dataset_calibrated_confidence_winnow_fdr.csv\"\n",
    ")\n",
    "decoy_metrics_df = pd.read_csv(\n",
    "    \"../results/helaqc_results/test_dataset_calibrated_confidence_dbg_fdr.csv\"\n",
    ")\n",
    "\n",
    "# -- Precision-Recall curve\n",
    "pr_fig = create_pr_curve_plot(\n",
    "    metadata,\n",
    "    \"Precision-recall curve for labelled data using database search\",\n",
    "    \"correct\",\n",
    ")\n",
    "pr_fig.savefig(\n",
    "    os.path.join(\n",
    "        output_dir, f\"{dataset_name}_labelled_precision_recall_with_db_search.png\"\n",
    "    ),\n",
    "    dpi=300,\n",
    "    bbox_inches=\"tight\",\n",
    ")\n",
    "pr_fig.savefig(\n",
    "    os.path.join(\n",
    "        output_dir, f\"{dataset_name}_labelled_precision_recall_with_db_search.pdf\"\n",
    "    ),\n",
    "    dpi=300,\n",
    "    bbox_inches=\"tight\",\n",
    ")\n",
    "plt.show()\n",
    "plt.close(pr_fig)\n",
    "\n",
    "# -- Calibration curves\n",
    "cal_fig = plot_combined_calibration_curves(\n",
    "    metadata, \"Calibration curves for labelled data using database search\", \"correct\"\n",
    ")\n",
    "cal_fig.savefig(\n",
    "    os.path.join(\n",
    "        output_dir, f\"{dataset_name}_labelled_calibration_curves_with_db_search.png\"\n",
    "    ),\n",
    "    dpi=300,\n",
    "    bbox_inches=\"tight\",\n",
    ")\n",
    "cal_fig.savefig(\n",
    "    os.path.join(\n",
    "        output_dir, f\"{dataset_name}_labelled_calibration_curves_with_db_search.pdf\"\n",
    "    ),\n",
    "    dpi=300,\n",
    "    bbox_inches=\"tight\",\n",
    ")\n",
    "plt.show()\n",
    "plt.close(cal_fig)\n",
    "\n",
    "# -- FDR accuracy\n",
    "fdr_fig = create_fdr_accuracy_plot(\n",
    "    metadata,\n",
    "    winnow_metrics_df,\n",
    "    decoy_metrics_df,\n",
    "    \"FDR accuracy for labelled data using database search\",\n",
    ")\n",
    "fdr_fig.savefig(\n",
    "    os.path.join(\n",
    "        output_dir, f\"{dataset_name}_labelled_fdr_accuracy_with_db_search.png\"\n",
    "    ),\n",
    "    dpi=300,\n",
    "    bbox_inches=\"tight\",\n",
    ")\n",
    "fdr_fig.savefig(\n",
    "    os.path.join(\n",
    "        output_dir, f\"{dataset_name}_labelled_fdr_accuracy_with_db_search.pdf\"\n",
    "    ),\n",
    "    dpi=300,\n",
    "    bbox_inches=\"tight\",\n",
    ")\n",
    "plt.show()\n",
    "plt.close(fdr_fig)\n",
    "\n",
    "confidence_fig = plot_combined_confidence_distributions(\n",
    "    metadata,\n",
    "    \"Confidence distribution for labelled data using database search\",\n",
    "    \"correct\",\n",
    ")\n",
    "fdr_fig.savefig(\n",
    "    os.path.join(\n",
    "        output_dir,\n",
    "        f\"{dataset_name}_labelled_confidence_distributions_with_db_search.png\",\n",
    "    ),\n",
    "    dpi=300,\n",
    "    bbox_inches=\"tight\",\n",
    ")\n",
    "fdr_fig.savefig(\n",
    "    os.path.join(\n",
    "        output_dir,\n",
    "        f\"{dataset_name}_labelled_confidence_distributions_with_db_search.pdf\",\n",
    "    ),\n",
    "    dpi=300,\n",
    "    bbox_inches=\"tight\",\n",
    ")\n",
    "plt.show()\n",
    "plt.close(confidence_fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# uv run scripts/map_peptides_to_proteomes.py --metadata-csv results/helaqc_results/test_dataset.csv --fasta-file fasta/human.fasta --output-csv results/helaqc_results/test_dataset.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load labelled test set\n",
    "metadata_path = \"../results/helaqc_results/test_dataset.csv\"\n",
    "output_dir = os.path.dirname(metadata_path) + \"/plots\"\n",
    "dataset_name = os.path.basename(metadata_path)\n",
    "metadata = pd.read_csv(metadata_path)\n",
    "\n",
    "# -- Load Winnow metrics\n",
    "winnow_metrics_df = pd.read_csv(\n",
    "    \"../results/helaqc_results/test_dataset_calibrated_confidence_winnow_fdr.csv\"\n",
    ")\n",
    "\n",
    "# -- Compute decoy metrics\n",
    "database_grounded_fdr_control = DatabaseGroundedFDRControl(\n",
    "    confidence_feature=\"calibrated_confidence\"\n",
    ")\n",
    "database_grounded_fdr_control.fit(\n",
    "    dataset=metadata, residue_masses=RESIDUE_MASSES, correct_column=\"proteome_hit\"\n",
    ")\n",
    "decoy_metrics_df = database_grounded_fdr_control.add_psm_fdr(\n",
    "    metadata, \"calibrated_confidence\"\n",
    ")\n",
    "decoy_metrics_df = decoy_metrics_df[[\"spectrum_id\", \"psm_fdr\"]]\n",
    "\n",
    "# -- Precision-Recall curve\n",
    "pr_fig = create_pr_curve_plot(\n",
    "    metadata,\n",
    "    \"Precision-recall curve for labelled data using proteome mapping\",\n",
    "    \"proteome_hit\",\n",
    ")\n",
    "pr_fig.savefig(\n",
    "    os.path.join(output_dir, f\"{dataset_name}_labelled_precision_recall.png\"),\n",
    "    dpi=300,\n",
    "    bbox_inches=\"tight\",\n",
    ")\n",
    "pr_fig.savefig(\n",
    "    os.path.join(output_dir, f\"{dataset_name}_labelled_precision_recall.pdf\"),\n",
    "    dpi=300,\n",
    "    bbox_inches=\"tight\",\n",
    ")\n",
    "plt.show()\n",
    "plt.close(pr_fig)\n",
    "\n",
    "# -- Calibration curves\n",
    "cal_fig = plot_combined_calibration_curves(\n",
    "    metadata,\n",
    "    \"Calibration curves for labelled data using proteome mapping\",\n",
    "    \"proteome_hit\",\n",
    ")\n",
    "cal_fig.savefig(\n",
    "    os.path.join(output_dir, f\"{dataset_name}_labelled_calibration_curves.png\"),\n",
    "    dpi=300,\n",
    "    bbox_inches=\"tight\",\n",
    ")\n",
    "cal_fig.savefig(\n",
    "    os.path.join(output_dir, f\"{dataset_name}_labelled_calibration_curves.pdf\"),\n",
    "    dpi=300,\n",
    "    bbox_inches=\"tight\",\n",
    ")\n",
    "plt.show()\n",
    "plt.close(cal_fig)\n",
    "\n",
    "# -- FDR accuracy\n",
    "fdr_fig = create_fdr_accuracy_plot(\n",
    "    metadata,\n",
    "    winnow_metrics_df,\n",
    "    decoy_metrics_df,\n",
    "    \"FDR accuracy for labelled data using proteome mapping\",\n",
    ")\n",
    "fdr_fig.savefig(\n",
    "    os.path.join(output_dir, f\"{dataset_name}_labelled_fdr_accuracy.png\"),\n",
    "    dpi=300,\n",
    "    bbox_inches=\"tight\",\n",
    ")\n",
    "fdr_fig.savefig(\n",
    "    os.path.join(output_dir, f\"{dataset_name}_labelled_fdr_accuracy.pdf\"),\n",
    "    dpi=300,\n",
    "    bbox_inches=\"tight\",\n",
    ")\n",
    "plt.show()\n",
    "plt.close(fdr_fig)\n",
    "\n",
    "confidence_fig = plot_combined_confidence_distributions(\n",
    "    metadata,\n",
    "    \"Confidence distribution for labelled data using proteome mapping\",\n",
    "    \"proteome_hit\",\n",
    ")\n",
    "confidence_fig.savefig(\n",
    "    os.path.join(output_dir, f\"{dataset_name}_labelled_confidence_distributions.png\"),\n",
    "    dpi=300,\n",
    "    bbox_inches=\"tight\",\n",
    ")\n",
    "confidence_fig.savefig(\n",
    "    os.path.join(output_dir, f\"{dataset_name}_labelled_confidence_distributions.pdf\"),\n",
    "    dpi=300,\n",
    "    bbox_inches=\"tight\",\n",
    ")\n",
    "plt.show()\n",
    "plt.close(confidence_fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# uv run scripts/map_peptides_to_proteomes.py --metadata-csv results/helaqc_results/raw_less_train.csv --fasta-file fasta/human.fasta --output-csv results/helaqc_results/raw_less_train.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load raw dataset\n",
    "metadata_path = \"../results/helaqc_results/raw_less_train.csv\"\n",
    "output_dir = os.path.dirname(metadata_path) + \"/plots\"\n",
    "dataset_name = os.path.basename(metadata_path)\n",
    "metadata = pd.read_csv(metadata_path)\n",
    "\n",
    "# -- Load Winnow metrics\n",
    "winnow_metrics_df = pd.read_csv(\n",
    "    \"../results/helaqc_results/raw_less_train_dataset_calibrated_confidence_winnow_fdr.csv\"\n",
    ")\n",
    "\n",
    "# -- Compute decoy metrics\n",
    "database_grounded_fdr_control = DatabaseGroundedFDRControl(\n",
    "    confidence_feature=\"calibrated_confidence\"\n",
    ")\n",
    "database_grounded_fdr_control.fit(\n",
    "    dataset=metadata, residue_masses=RESIDUE_MASSES, correct_column=\"proteome_hit\"\n",
    ")\n",
    "decoy_metrics_df = database_grounded_fdr_control.add_psm_fdr(\n",
    "    metadata, \"calibrated_confidence\"\n",
    ")\n",
    "decoy_metrics_df = decoy_metrics_df[[\"spectrum_id\", \"psm_fdr\"]]\n",
    "\n",
    "# -- Precision-Recall curve\n",
    "pr_fig = create_pr_curve_plot(\n",
    "    metadata,\n",
    "    \"Precision-recall curve for raw data using proteome mapping\",\n",
    "    \"proteome_hit\",\n",
    ")\n",
    "pr_fig.savefig(\n",
    "    os.path.join(output_dir, f\"{dataset_name}_raw_precision_recall.png\"),\n",
    "    dpi=300,\n",
    "    bbox_inches=\"tight\",\n",
    ")\n",
    "pr_fig.savefig(\n",
    "    os.path.join(output_dir, f\"{dataset_name}_raw_precision_recall.pdf\"),\n",
    "    dpi=300,\n",
    "    bbox_inches=\"tight\",\n",
    ")\n",
    "plt.show()\n",
    "plt.close(pr_fig)\n",
    "\n",
    "# -- Calibration curves\n",
    "cal_fig = plot_combined_calibration_curves(\n",
    "    metadata, \"Calibration curves for raw data using proteome mapping\", \"proteome_hit\"\n",
    ")\n",
    "cal_fig.savefig(\n",
    "    os.path.join(output_dir, f\"{dataset_name}_raw_calibration_curves.png\"),\n",
    "    dpi=300,\n",
    "    bbox_inches=\"tight\",\n",
    ")\n",
    "cal_fig.savefig(\n",
    "    os.path.join(output_dir, f\"{dataset_name}_raw_calibration_curves.pdf\"),\n",
    "    dpi=300,\n",
    "    bbox_inches=\"tight\",\n",
    ")\n",
    "plt.show()\n",
    "plt.close(cal_fig)\n",
    "\n",
    "# -- FDR accuracy\n",
    "fdr_fig = create_fdr_accuracy_plot(\n",
    "    metadata,\n",
    "    winnow_metrics_df,\n",
    "    decoy_metrics_df,\n",
    "    \"FDR accuracy for raw data using proteome mapping\",\n",
    ")\n",
    "fdr_fig.savefig(\n",
    "    os.path.join(output_dir, f\"{dataset_name}_raw_fdr_accuracy.png\"),\n",
    "    dpi=300,\n",
    "    bbox_inches=\"tight\",\n",
    ")\n",
    "fdr_fig.savefig(\n",
    "    os.path.join(output_dir, f\"{dataset_name}_raw_fdr_accuracy.pdf\"),\n",
    "    dpi=300,\n",
    "    bbox_inches=\"tight\",\n",
    ")\n",
    "plt.show()\n",
    "plt.close(fdr_fig)\n",
    "\n",
    "confidence_fig = plot_combined_confidence_distributions(\n",
    "    metadata,\n",
    "    \"Confidence distribution for raw data using proteome mapping\",\n",
    "    \"proteome_hit\",\n",
    ")\n",
    "confidence_fig.savefig(\n",
    "    os.path.join(output_dir, f\"{dataset_name}_raw_confidence_distributions.png\"),\n",
    "    dpi=300,\n",
    "    bbox_inches=\"tight\",\n",
    ")\n",
    "confidence_fig.savefig(\n",
    "    os.path.join(output_dir, f\"{dataset_name}_raw_confidence_distributions.pdf\"),\n",
    "    dpi=300,\n",
    "    bbox_inches=\"tight\",\n",
    ")\n",
    "plt.show()\n",
    "plt.close(confidence_fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FDR Metrics for External Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _map_l_to_i_in_sequences(metadata: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Map L to I in sequences and predictions.\"\"\"\n",
    "    logger.info(\"Mapping L to I in sequences and predictions\")\n",
    "\n",
    "    def _replace_l_with_i(value):\n",
    "        \"\"\"Replace L with I in a value, handling both strings and lists.\"\"\"\n",
    "        if isinstance(value, str):\n",
    "            return value.replace(\"L\", \"I\")\n",
    "        elif isinstance(value, list):\n",
    "            return [\n",
    "                token.replace(\"L\", \"I\") if isinstance(token, str) else token\n",
    "                for token in value\n",
    "            ]\n",
    "        return value\n",
    "\n",
    "    for col in [\"sequence\", \"prediction\"]:\n",
    "        if col in metadata.columns:\n",
    "            metadata[col] = metadata[col].apply(_replace_l_with_i)\n",
    "\n",
    "    return metadata\n",
    "\n",
    "\n",
    "def _convert_object_columns(metadata: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Convert object columns that might contain string representations of Python objects.\"\"\"\n",
    "\n",
    "    def try_convert(value):\n",
    "        try:\n",
    "            return ast.literal_eval(value)\n",
    "        except (ValueError, SyntaxError):\n",
    "            return value  # Return original if conversion fails\n",
    "\n",
    "    # Apply conversion to object (string) columns\n",
    "    for col in metadata.select_dtypes(include=[\"object\"]).columns:\n",
    "        metadata[col] = metadata[col].apply(try_convert)\n",
    "\n",
    "    return metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## C. elegans (PXD014877)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Labelled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labelled_df = pd.read_csv(\"../new_model/results/labelled_PXD014877_results.csv\")\n",
    "\n",
    "# Convert object columns\n",
    "labelled_df = _convert_object_columns(labelled_df)\n",
    "\n",
    "# Map L to I in sequences and predictions\n",
    "labelled_df = _map_l_to_i_in_sequences(labelled_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "non_parametric_fdr_control = NonParametricFDRControl()\n",
    "non_parametric_fdr_control.fit(dataset=labelled_df[\"confidence\"])\n",
    "\n",
    "logger.info(\n",
    "    f\"Winnow FDR threshold for raw confidence: {non_parametric_fdr_control.get_confidence_cutoff(threshold=0.05)}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "non_parametric_fdr_control = NonParametricFDRControl()\n",
    "non_parametric_fdr_control.fit(dataset=labelled_df[\"calibrated_confidence\"])\n",
    "\n",
    "logger.info(\n",
    "    f\"Winnow FDR threshold for calibrated confidence: {non_parametric_fdr_control.get_confidence_cutoff(threshold=0.05)}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "database_grounded_fdr_control = DatabaseGroundedFDRControl(\n",
    "    confidence_feature=\"confidence\"\n",
    ")\n",
    "database_grounded_fdr_control.fit(dataset=labelled_df, residue_masses=RESIDUE_MASSES)\n",
    "\n",
    "logger.info(\n",
    "    f\"Database-grounded FDR threshold for raw confidence: {database_grounded_fdr_control.get_confidence_cutoff(threshold=0.05)}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "database_grounded_fdr_control = DatabaseGroundedFDRControl(\n",
    "    confidence_feature=\"calibrated_confidence\"\n",
    ")\n",
    "database_grounded_fdr_control.fit(dataset=labelled_df, residue_masses=RESIDUE_MASSES)\n",
    "\n",
    "logger.info(\n",
    "    f\"Database-grounded FDR threshold for calibrated confidence: {database_grounded_fdr_control.get_confidence_cutoff(threshold=0.05)}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "de_novo_df = pd.read_csv(\"../new_model/results/raw_PXD014877_results.csv\")\n",
    "\n",
    "# Convert object columns\n",
    "de_novo_df = _convert_object_columns(de_novo_df)\n",
    "\n",
    "# Map L to I in sequences and predictions\n",
    "de_novo_df = _map_l_to_i_in_sequences(de_novo_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "non_parametric_fdr_control = NonParametricFDRControl()\n",
    "non_parametric_fdr_control.fit(dataset=de_novo_df[\"confidence\"])\n",
    "\n",
    "logger.info(\n",
    "    f\"Winnow FDR threshold for raw confidence: {non_parametric_fdr_control.get_confidence_cutoff(threshold=0.05)}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "non_parametric_fdr_control = NonParametricFDRControl()\n",
    "non_parametric_fdr_control.fit(dataset=de_novo_df[\"calibrated_confidence\"])\n",
    "\n",
    "logger.info(\n",
    "    f\"Winnow FDR threshold for calibrated confidence: {non_parametric_fdr_control.get_confidence_cutoff(threshold=0.05)}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Immuno-2 (PXD023064)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Labelled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labelled_df = pd.read_csv(\"../new_model/results/labelled_PXD023064_results.csv\")\n",
    "\n",
    "# Convert object columns\n",
    "labelled_df = _convert_object_columns(labelled_df)\n",
    "\n",
    "# Map L to I in sequences and predictions\n",
    "labelled_df = _map_l_to_i_in_sequences(labelled_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "non_parametric_fdr_control = NonParametricFDRControl()\n",
    "non_parametric_fdr_control.fit(dataset=labelled_df[\"confidence\"])\n",
    "\n",
    "logger.info(\n",
    "    f\"Winnow FDR threshold for raw confidence: {non_parametric_fdr_control.get_confidence_cutoff(threshold=0.05)}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "non_parametric_fdr_control = NonParametricFDRControl()\n",
    "non_parametric_fdr_control.fit(dataset=labelled_df[\"calibrated_confidence\"])\n",
    "\n",
    "logger.info(\n",
    "    f\"Winnow FDR threshold for calibrated confidence: {non_parametric_fdr_control.get_confidence_cutoff(threshold=0.05)}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "database_grounded_fdr_control = DatabaseGroundedFDRControl(\n",
    "    confidence_feature=\"confidence\"\n",
    ")\n",
    "database_grounded_fdr_control.fit(dataset=labelled_df, residue_masses=RESIDUE_MASSES)\n",
    "\n",
    "logger.info(\n",
    "    f\"Database-grounded FDR threshold for raw confidence: {database_grounded_fdr_control.get_confidence_cutoff(threshold=0.05)}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "database_grounded_fdr_control = DatabaseGroundedFDRControl(\n",
    "    confidence_feature=\"calibrated_confidence\"\n",
    ")\n",
    "database_grounded_fdr_control.fit(dataset=labelled_df, residue_masses=RESIDUE_MASSES)\n",
    "\n",
    "logger.info(\n",
    "    f\"Database-grounded FDR threshold for calibrated confidence: {database_grounded_fdr_control.get_confidence_cutoff(threshold=0.05)}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "de_novo_df = pd.read_csv(\"../new_model/results/raw_PXD023064_results.csv\")\n",
    "\n",
    "# Convert object columns\n",
    "de_novo_df = _convert_object_columns(de_novo_df)\n",
    "\n",
    "# Map L to I in sequences and predictions\n",
    "de_novo_df = _map_l_to_i_in_sequences(de_novo_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "non_parametric_fdr_control = NonParametricFDRControl()\n",
    "non_parametric_fdr_control.fit(dataset=de_novo_df[\"confidence\"])\n",
    "\n",
    "logger.info(\n",
    "    f\"Winnow FDR threshold for raw confidence: {non_parametric_fdr_control.get_confidence_cutoff(threshold=0.05)}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "non_parametric_fdr_control = NonParametricFDRControl()\n",
    "non_parametric_fdr_control.fit(dataset=de_novo_df[\"calibrated_confidence\"])\n",
    "\n",
    "logger.info(\n",
    "    f\"Winnow FDR threshold for calibrated confidence: {non_parametric_fdr_control.get_confidence_cutoff(threshold=0.05)}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## General model test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labelled_df = pd.read_csv(\"../new_model/results/labelled_general_results.csv\")\n",
    "\n",
    "# Convert object columns\n",
    "labelled_df = _convert_object_columns(labelled_df)\n",
    "\n",
    "# Map L to I in sequences and predictions\n",
    "labelled_df = _map_l_to_i_in_sequences(labelled_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "non_parametric_fdr_control = NonParametricFDRControl()\n",
    "non_parametric_fdr_control.fit(dataset=labelled_df[\"confidence\"])\n",
    "\n",
    "logger.info(\n",
    "    f\"Winnow FDR threshold for raw confidence: {non_parametric_fdr_control.get_confidence_cutoff(threshold=0.05)}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "non_parametric_fdr_control = NonParametricFDRControl()\n",
    "non_parametric_fdr_control.fit(dataset=labelled_df[\"calibrated_confidence\"])\n",
    "\n",
    "logger.info(\n",
    "    f\"Winnow FDR threshold for calibrated confidence: {non_parametric_fdr_control.get_confidence_cutoff(threshold=0.05)}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "database_grounded_fdr_control = DatabaseGroundedFDRControl(\n",
    "    confidence_feature=\"confidence\"\n",
    ")\n",
    "database_grounded_fdr_control.fit(dataset=labelled_df, residue_masses=RESIDUE_MASSES)\n",
    "\n",
    "logger.info(\n",
    "    f\"Database-grounded FDR threshold for raw confidence: {database_grounded_fdr_control.get_confidence_cutoff(threshold=0.05)}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "database_grounded_fdr_control = DatabaseGroundedFDRControl(\n",
    "    confidence_feature=\"calibrated_confidence\"\n",
    ")\n",
    "database_grounded_fdr_control.fit(dataset=labelled_df, residue_masses=RESIDUE_MASSES)\n",
    "\n",
    "logger.info(\n",
    "    f\"Database-grounded FDR threshold for calibrated confidence: {database_grounded_fdr_control.get_confidence_cutoff(threshold=0.05)}\"\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
